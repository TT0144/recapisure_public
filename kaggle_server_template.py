%%writefile kaggle_api_server.py
"""
Kaggle Notebookç”¨ Apertus-8B APIã‚µãƒ¼ãƒãƒ¼

âš ï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Kaggle Notebookã«ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„
âš ï¸ ä»¥ä¸‹ã®å¤‰æ•°ã‚’è‡ªåˆ†ã®å€¤ã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼š
    - API_KEY: è‡ªåˆ†ã§ç”Ÿæˆã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãªAPIã‚­ãƒ¼ï¼ˆ32æ–‡å­—ä»¥ä¸Šæ¨å¥¨ï¼‰
    - hf_token: HuggingFaceã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
"""
from flask import Flask, request, jsonify
import torch
import os
import time
from functools import wraps

# â­ é€²æ—ãƒãƒ¼ã¨ãƒ­ã‚°ã‚’æŠ‘åˆ¶
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
import logging
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("accelerate").setLevel(logging.ERROR)

app = Flask(__name__)

tokenizer = None
model = None

# ğŸ”’ é‡è¦: ã“ã®APIã‚­ãƒ¼ã‚’è‡ªåˆ†ã§ç”Ÿæˆã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—åˆ—ã«ç½®ãæ›ãˆã¦ãã ã•ã„
# ä¾‹: import secrets; print(secrets.token_urlsafe(32))
API_KEY = "YOUR_RANDOM_API_KEY_HERE_REPLACE_ME"

def require_api_key(f):
    """APIèªè¨¼ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({"success": False, "error": "Unauthorized"}), 401
        token = auth_header.replace('Bearer ', '')
        if token != API_KEY:
            return jsonify({"success": False, "error": "Invalid API key"}), 403
        return f(*args, **kwargs)
    return decorated_function

def load_model():
    """Apertus-8Bãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    global tokenizer, model
    print("ğŸ” Apertus-8B ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # ğŸ”’ é‡è¦: HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã‚’https://huggingface.co/settings/tokensã§å–å¾—ã—ã¦ç½®ãæ›ãˆã¦ãã ã•ã„
    hf_token = "YOUR_HUGGINGFACE_TOKEN_HERE"
    model_id = "swiss-ai/Apertus-8B-Instruct-2509"
    
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import transformers
    transformers.logging.set_verbosity_error()
    
    print("ğŸ“¦ Tokenizerãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_id, token=hf_token, trust_remote_code=True
        )
    except Exception as e:
        print(f"âš ï¸ AutoTokenizerå¤±æ•—: {e}")
        from transformers import PreTrainedTokenizerFast
        from huggingface_hub import hf_hub_download
        tokenizer_file = hf_hub_download(
            repo_id=model_id, filename="tokenizer.json", token=hf_token
        )
        tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)
        if not tokenizer.pad_token:
            tokenizer.pad_token = tokenizer.eos_token
    
    print("ğŸ“¦ Apertus-8Bãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­... (3-5åˆ†)")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        token=hf_token,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    model.eval()
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
    print(f"ğŸ“Š dtype: {model.dtype}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ VRAMä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.1f} GB")

def build_instruct_prompt(task: str, content: str, language: str = "Japanese") -> str:
    """Apertus-8B Instructå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
    
    native_system_prompts = {
        "Japanese": """ã‚ãªãŸã¯Swiss AIãŒé–‹ç™ºã—ãŸå¤šè¨€èªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ŒApertusã€ã§ã™ã€‚
1,811è¨€èªã«å¯¾å¿œã—ã¦ãŠã‚Šã€è¦ç´„ã¨ç¿»è¨³ãŒå¾—æ„ã§ã™ã€‚

ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
- å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã†ã“ã¨
- è‹±èªã‚„ãã®ä»–ã®è¨€èªã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢
- è¦ç´„çµæœã®ã¿ã‚’å‡ºåŠ›ã—ã€ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦""",

        "English": """You are Apertus, a multilingual AI assistant developed by Swiss AI.
Supporting 1,811 languages, specializing in summarization and translation.

ã€Critical Rulesã€‘
- Always respond in English
- No other languages allowed
- Output only the summary without extra explanations""",

        "Chinese": """ä½ æ˜¯ç”±Swiss AIå¼€å‘çš„å¤šè¯­è¨€AIåŠ©æ‰‹ã€ŒApertusã€ã€‚
æ”¯æŒ1,811ç§è¯­è¨€ï¼Œæ“…é•¿æ‘˜è¦å’Œç¿»è¯‘ã€‚

ã€æœ€é‡è¦è§„åˆ™ã€‘
- å¿…é¡»ç”¨ä¸­æ–‡å›ç­”
- ç¦æ­¢ä½¿ç”¨å…¶ä»–è¯­è¨€
- åªè¾“å‡ºæ‘˜è¦ç»“æœï¼Œæ— éœ€é¢å¤–è¯´æ˜""",
    }
    
    system_prompt = native_system_prompts.get(language, native_system_prompts["Japanese"])
    
    user_content = f"{task}\n\n{content}"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]
    
    try:
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        return prompt
    except Exception as e:
        print(f"âš ï¸ Chat templateé©ç”¨å¤±æ•—: {e}")
        return f"{system_prompt}\n\n{user_content}"

@app.route('/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    global model, tokenizer
    return jsonify({
        "success": True,
        "status": "running",
        "model_loaded": model is not None and tokenizer is not None,
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A",
        "model": "swiss-ai/Apertus-8B-Instruct"
    })

@app.route('/summarize', methods=['POST'])
@require_api_key
def summarize_text():
    """è¦ç´„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return jsonify({"success": False, "error": "Model not loaded"}), 503
    
    try:
        data = request.get_json()
        text = data.get('text', '')
        max_length = data.get('max_length', 400)
        source_lang = data.get('source_lang', 'auto-detect')
        target_lang = data.get('target_lang', 'Japanese')
        style = data.get('style', 'balanced')
        summary_mode = data.get('summary_mode', 'short')
        
        if not text:
            return jsonify({"success": False, "error": "No text provided"}), 400
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ•°åˆ¶é™
        if len(text) > 10000:
            text = text[:10000]
        
        # è¦ç´„ã‚¿ã‚¹ã‚¯æ§‹ç¯‰
        if summary_mode == 'long':
            task = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{target_lang}ã§800-1000æ–‡å­—ã§è©³ç´°ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„ã‚¹ã‚¿ã‚¤ãƒ«: {style}ã€‘
- balanced: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ¨™æº–çš„ãªè¦ç´„
- detailed: ã‚ˆã‚Šè©³ç´°ã§åŒ…æ‹¬çš„ãªè¦ç´„
- concise: ç°¡æ½”ã§è¦ç‚¹ã‚’çµã£ãŸè¦ç´„
- tech_doc: æŠ€è¡“æ–‡æ›¸å‘ã‘ã®å°‚é–€çš„ãªè¦ç´„

é‡è¦: è¦ç´„çµæœã®ã¿ã‚’å‡ºåŠ›ã—ã€å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""
        else:
            task = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{target_lang}ã§200-400æ–‡å­—ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„ã‚¹ã‚¿ã‚¤ãƒ«: {style}ã€‘

é‡è¦: è¦ç´„çµæœã®ã¿ã‚’å‡ºåŠ›ã—ã€å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""
        
        prompt = build_instruct_prompt(task, text, target_lang)
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length * 3,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
        if "<|assistant|>" in generated_text:
            summary = generated_text.split("<|assistant|>")[-1].strip()
        else:
            summary = generated_text[len(prompt):].strip()
        
        return jsonify({
            "success": True,
            "summary": summary,
            "model": "Apertus-8B-Instruct",
            "source_lang": source_lang,
            "target_lang": target_lang
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/expand', methods=['POST'])
@require_api_key
def expand_text():
    """æ–‡ç« å±•é–‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return jsonify({"success": False, "error": "Model not loaded"}), 503
    
    try:
        data = request.get_json()
        text = data.get('text', '')
        target_length = data.get('target_length', 500)
        target_lang = data.get('target_lang', 'Japanese')
        
        if not text:
            return jsonify({"success": False, "error": "No text provided"}), 400
        
        task = f"""ä»¥ä¸‹ã®çŸ­æ–‡ã‚’{target_lang}ã§{target_length}æ–‡å­—ç¨‹åº¦ã«è©³ç´°ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚

é‡è¦: å±•é–‹çµæœã®ã¿ã‚’å‡ºåŠ›ã—ã€å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""
        
        prompt = build_instruct_prompt(task, text, target_lang)
        
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=target_length * 3,
                temperature=0.8,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if "<|assistant|>" in generated_text:
            result = generated_text.split("<|assistant|>")[-1].strip()
        else:
            result = generated_text[len(prompt):].strip()
        
        return jsonify({
            "success": True,
            "result": result,
            "model": "Apertus-8B-Instruct"
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
load_model()

# ngrokã¾ãŸã¯pyngrokã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆ
print("\nğŸš€ Flaskã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¾ã™...")
print("âš ï¸ ngrokã§ãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆã—ã¦URLã‚’å–å¾—ã—ã¦ãã ã•ã„\n")

# Kaggleã§ã¯app.run()ã‚’ä½¿ç”¨
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
