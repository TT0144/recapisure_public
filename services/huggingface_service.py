#!/usr/bin/env python3
"""
Hugging Face Transformers Service
å®Œå…¨ç„¡æ–™ãƒ»APIã‚­ãƒ¼ä¸è¦ã®ç¿»è¨³ãƒ»è¦ç´„ã‚µãƒ¼ãƒ“ã‚¹
"""

import os
import logging
import re
import time
from typing import Optional, Dict, Any, List, Tuple
from dataclasses import dataclass
from enum import Enum

from database import get_db

logger = logging.getLogger(__name__)

# â­ ãƒ™ãƒ¼ã‚¹ã®å°‚é–€ç”¨èªè£œæ­£ï¼ˆæœ€ä½é™ã®æ—¢çŸ¥å¤‰æ›ï¼‰
DEFAULT_JP_TERM_CORRECTIONS: Dict[str, str] = {
    "ç†ŠçŒ«": "ã‚¯ãƒ¼ã‚¬ãƒ¼",
    "ã‚¸ãƒ£ã‚¬ã‚¤ã‚¢": "ã‚¸ãƒ£ã‚¬ãƒ¼",
}

# Hugging Faceãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ(é…å»¶èª­ã¿è¾¼ã¿)
HF_AVAILABLE = False
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
    import torch
    HF_AVAILABLE = True
    logger.info("âœ… Hugging Face Transformersåˆ©ç”¨å¯èƒ½")
except ImportError:
    logger.warning("âš ï¸ Hugging Face TransformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")


class TaskType(Enum):
    """ã‚¿ã‚¹ã‚¯ã®ç¨®é¡"""
    SUMMARIZE = "summarize"
    EXPAND = "expand"
    TRANSLATE = "translate"


@dataclass
class HFResponse:
    """Hugging Faceãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    result: str
    model_used: str
    execution_time: float = 0.0
    confidence: float = 0.85
    token_usage: Dict[str, int] = None
    error: Optional[str] = None
    # â­ AIå“è³ªåˆ†æãƒ‡ãƒ¼ã‚¿ï¼ˆå°±æ´»ã‚¢ãƒ”ãƒ¼ãƒ«ç”¨ï¼‰
    quality_metrics: Optional[Dict[str, Any]] = None


class HuggingFaceService:
    """Hugging Faceç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.available = HF_AVAILABLE
        self.models: Dict[str, Any] = {}
        self._dictionary_cache: Dict[Tuple[Optional[str], Optional[str]], Dict[str, Any]] = {}
        self._dictionary_cache_ttl = 60  # seconds
        self._default_term_corrections: Dict[str, str] = dict(DEFAULT_JP_TERM_CORRECTIONS)
        
        if HF_AVAILABLE:
            # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š(GPUä½¿ç”¨å¯èƒ½ãªã‚‰GPUã€ãªã‘ã‚Œã°CPU)
            self.device = 0 if torch.cuda.is_available() else -1
            logger.info(f"ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹: {'GPU' if self.device >= 0 else 'CPU'}")
            
            # âš¡âš¡âš¡ CPUæœ€é©åŒ–: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’åˆ¶é™ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚¤ãƒƒãƒå‰Šæ¸›ï¼‰
            if self.device < 0:
                torch.set_num_threads(2)  # â­ 4â†’2: ã•ã‚‰ã«é«˜é€ŸåŒ–
                logger.info("âš¡ CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°: 2ï¼ˆè¶…é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰")
            
            # â­ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®š
            if self.device == -1:  # CPUç’°å¢ƒ
                # CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æœ€é©åŒ–
                torch.set_num_threads(4)  # ä¸¦åˆ—å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’åˆ¶é™
                logger.info("âš™ï¸ CPUæœ€é©åŒ–: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°=4")
        else:
            self.device = -1
    
    def _get_user_dictionary_terms(
        self,
        source_lang: Optional[str] = None,
        target_lang: Optional[str] = None,
    ) -> Dict[str, str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‹ã‚‰ç”¨èªã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        cache_key = (source_lang, target_lang)
        cached = self._dictionary_cache.get(cache_key)
        now = time.time()

        if cached:
            timestamp = cached.get("timestamp", 0)
            if now - timestamp < self._dictionary_cache_ttl:
                return cached.get("terms", {})

        terms: Dict[str, str] = {}
        try:
            db = get_db()
            entries = db.get_user_dictionary(source_lang=source_lang, target_lang=target_lang)
            for entry in entries:
                source = entry.get("source_term", "").strip()
                target = entry.get("target_term", "").strip()
                if source and target:
                    terms[source] = target
        except Exception as exc:
            logger.warning(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸å–å¾—ã«å¤±æ•—: {exc}")
            terms = {}

        self._dictionary_cache[cache_key] = {"timestamp": now, "terms": terms}
        return terms

    def _collect_term_corrections(
        self,
        source_lang: Optional[str] = None,
        target_lang: Optional[str] = None,
    ) -> Dict[str, str]:
        """åŸºæœ¬è¾æ›¸ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã‚’ãƒãƒ¼ã‚¸ã—ã¦è£œæ­£ç”¨è¾æ›¸ã‚’ç”Ÿæˆ"""
        corrections: Dict[str, str] = dict(self._default_term_corrections)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã®ã¿æŒ‡å®šã®ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸
        if target_lang:
            corrections.update(self._get_user_dictionary_terms(source_lang=None, target_lang=target_lang))

        # ã‚½ãƒ¼ã‚¹ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµ„ã¿åˆã‚ã›
        corrections.update(self._get_user_dictionary_terms(source_lang=source_lang, target_lang=target_lang))

        # Noneã‚­ãƒ¼ãªã©ã‚’é™¤å¤–
        return {k: v for k, v in corrections.items() if k and v}

    def invalidate_dictionary_cache(
        self,
        source_lang: Optional[str] = None,
        target_lang: Optional[str] = None,
    ) -> None:
        """æŒ‡å®šã—ãŸæ¡ä»¶ã®è¾æ›¸ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–"""
        if source_lang is None and target_lang is None:
            self._dictionary_cache.clear()
            return

        key = (source_lang, target_lang)
        self._dictionary_cache.pop(key, None)
        if target_lang:
            self._dictionary_cache.pop((None, target_lang), None)

    def _get_summarization_pipeline(self):
        """è¦ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å–å¾—(ã‚­ãƒ£ãƒƒã‚·ãƒ¥)"""
        if 'summarization' not in self.models:
            try:
                # è»½é‡é«˜é€Ÿãªè¦ç´„ãƒ¢ãƒ‡ãƒ«
                logger.info("ğŸ“¥ è¦ç´„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                self.models['summarization'] = pipeline(
                    "summarization",
                    model="sshleifer/distilbart-cnn-12-6",  # è»½é‡ãƒ»é«˜é€Ÿç‰ˆBART (ç´„1/4ã®ã‚µã‚¤ã‚º)
                    device=self.device
                )
                logger.info("âœ… è¦ç´„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            except Exception as e:
                logger.error(f"âŒ è¦ç´„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return None
        return self.models['summarization']

    def _get_japanese_summarization_pipeline(self):
        """æ—¥æœ¬èªå°‚ç”¨ã®è¦ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å–å¾—(ã‚­ãƒ£ãƒƒã‚·ãƒ¥)
        
        mBART-large-50ã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªâ†’æ—¥æœ¬èªã®è¦ç´„ã‚’å®Ÿç¾
        """
        if 'summarization_jp' not in self.models:
            try:
                logger.info("ğŸ“¥ æ—¥æœ¬èªå¤šè¨€èªè¦ç´„ãƒ¢ãƒ‡ãƒ«(mBART)ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
                
                model_name = "facebook/mbart-large-50"
                tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang="ja_XX", tgt_lang="ja_XX")
                model = MBartForConditionalGeneration.from_pretrained(model_name)
                
                # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                if self.device >= 0:
                    model = model.cuda()
                
                # ã‚«ã‚¹ã‚¿ãƒ è¦ç´„é–¢æ•°ã‚’ä¿å­˜
                self.models['summarization_jp'] = {
                    'tokenizer': tokenizer,
                    'model': model,
                    'is_mbart': True
                }
                logger.info("âœ… æ—¥æœ¬èªå¤šè¨€èªè¦ç´„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (mBART-50)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ—¥æœ¬èªè¦ç´„ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
                # ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ None ã‚’è¿”ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨±å¯
                return None
        return self.models.get('summarization_jp')
    
    def _get_translation_pipeline(self):
        """ç¿»è¨³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å–å¾—(è‹±èªâ†’æ—¥æœ¬èª)"""
        if 'translation' not in self.models:
            try:
                logger.info("ğŸ“¥ ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                # Meta NLLBãƒ¢ãƒ‡ãƒ« - é«˜å“è³ªãªå¤šè¨€èªç¿»è¨³
                from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
                
                model_name = "facebook/nllb-200-distilled-600M"
                tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang="eng_Latn", tgt_lang="jpn_Jpan")
                model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
                
                # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                if self.device >= 0:
                    model = model.cuda()
                
                self.models['translation'] = {
                    'tokenizer': tokenizer,
                    'model': model,
                    'model_name': model_name,
                    'src_lang': 'eng_Latn',
                    'tgt_lang': 'jpn_Jpan'
                }
                logger.info("âœ… ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (NLLB-200)")
            except Exception as e:
                logger.error(f"âŒ ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Helsinki-NLPãƒ¢ãƒ‡ãƒ«
                try:
                    logger.info("ğŸ“¥ ä»£æ›¿ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                    self.models['translation'] = pipeline(
                        "translation_en_to_ja",
                        model="Helsinki-NLP/opus-mt-en-ja",
                        device=self.device
                    )
                    logger.info("âœ… ä»£æ›¿ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                except Exception as e2:
                    logger.error(f"âŒ ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚‚ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e2}")
                    return None
        return self.models['translation']
    
    def _extract_proper_nouns(self, text: str) -> List[Tuple[str, str]]:
        """
        â­ å›ºæœ‰åè©ã‚’æŠ½å‡ºï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ + æœ€å°é™ã®é‡è¦è¾æ›¸ï¼‰
        
        è¾æ›¸ã‚’å¢—ã‚„ã™ã®ã§ã¯ãªãã€æ±ç”¨çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã§å¯¾å¿œ
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            [(å›ºæœ‰åè©, ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼), ...] ã®ãƒªã‚¹ãƒˆ
        """
        proper_nouns = []
        seen_nouns = set()  # é‡è¤‡é™¤å»ç”¨
        
        # â­ æœ€å°é™ã®é‡è¦è¾æ›¸ï¼ˆé »å‡ºã™ã‚‹èª¤è¨³ã—ã‚„ã™ã„å˜èªã®ã¿ï¼‰
        # å‹•ç‰©åã¯ç‰¹ã«èª¤è¨³ã•ã‚Œã‚„ã™ã„ã®ã§ä¿æŒ
        critical_terms = {
            # å‹•ç‰©åï¼ˆèª¤è¨³ã•ã‚Œã‚„ã™ã„ï¼‰
            'cougar', 'puma', 'mountain lion', 'jaguar', 'panther', 'leopard',
            'moose', 'elk', 'bison', 'grizzly', 'wolf', 'bear', 'deer',
            # é‡è¦ãªç•¥èªï¼ˆçµ¶å¯¾ã«ä¿è­·ã™ã¹ãï¼‰
            'COVID-19', 'SARS-CoV-2', 'DNA', 'RNA', 'HIV', 'AIDS',
            'NASA', 'WHO', 'FBI', 'CIA', 'UN', 'EU', 'NATO',
            # è©¦é¨“åï¼ˆå­¦è¡“ç³»PDFã§é »å‡ºï¼‰
            'TOEFL', 'IELTS', 'SAT', 'GRE', 'GMAT', 'TOEIC'
        }
        
        def add_noun(noun: str):
            """å›ºæœ‰åè©ã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
            if noun and noun not in seen_nouns and len(noun) > 1:
                seen_nouns.add(noun)
                # â­â­â­ æ•°å­—ã®ã¿ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãŒçµ¶å¯¾ã«è§¦ã‚‰ãªã„ï¼‰
                # è¨˜å·ã‚„æ–‡å­—ã‚’ä½¿ã†ã¨ç¿»è¨³ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ç´”ç²‹ãªæ•°å­—ã®ã¿
                placeholder = f"__NOUN{len(proper_nouns):03d}__"
                proper_nouns.append((noun, placeholder))
                return True
            return False
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³0: é‡è¦è¾æ›¸ã‹ã‚‰ã®æŠ½å‡ºï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
        for term in critical_terms:
            pattern = r'\b' + re.escape(term) + r'\b'
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                add_noun(match.group())
        
        # â­ ãƒ‘ã‚¿ãƒ¼ãƒ³1: å¤§æ–‡å­—ã§å§‹ã¾ã‚‹é€£ç¶šã—ãŸå˜èªï¼ˆäººåãƒ»åœ°åãƒ»çµ„ç¹”åï¼‰
        # ä¾‹: "Albert Einstein", "New York City", "Microsoft Corporation"
        pattern1 = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b'
        matches1 = re.finditer(pattern1, text)
        for match in matches1:
            add_noun(match.group())
        
        # â­ ãƒ‘ã‚¿ãƒ¼ãƒ³2: å…¨ã¦å¤§æ–‡å­—ã®ç•¥èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        # ä¾‹: "NASA", "FBI", "AI", "ML", "IoT"
        pattern2 = r'\b[A-Z]{2,}\b'
        matches2 = re.finditer(pattern2, text)
        for match in matches2:
            add_noun(match.group())
        
        # â­ ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ•°å­—ã‚’å«ã‚€å›ºæœ‰åè©ï¼ˆè£½å“åã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç­‰ï¼‰
        # ä¾‹: "GPT-4", "Windows 11", "COVID-19", "iPhone 15"
        pattern3 = r'\b[A-Z][A-Za-z0-9]*[-\s]?\d+[A-Za-z0-9]*\b'
        matches3 = re.finditer(pattern3, text)
        for match in matches3:
            add_noun(match.group())
        
        # â­ ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒã‚¤ãƒ•ãƒ³/ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ä»˜ãã®å°‚é–€ç”¨èª
        # ä¾‹: "SARS-CoV-2", "mRNA-1273", "deep-learning"
        pattern4 = r'\b[A-Za-z]+[-_][A-Za-z0-9]+(?:[-_][A-Za-z0-9]+)*\b'
        matches4 = re.finditer(pattern4, text)
        for match in matches4:
            noun = match.group()
            # 3æ–‡å­—ä»¥ä¸Šã®å ´åˆã®ã¿ï¼ˆ"a-b"ã®ã‚ˆã†ãªçŸ­ã„ã‚‚ã®ã¯é™¤å¤–ï¼‰
            if len(noun) >= 5:
                add_noun(noun)
        
        # â­ ãƒ‘ã‚¿ãƒ¼ãƒ³5: URLã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆãã®ã¾ã¾ä¿è­·ï¼‰
        pattern5 = r'\b(?:https?://|www\.)[^\s]+\b|\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        matches5 = re.finditer(pattern5, text)
        for match in matches5:
            add_noun(match.group())
        
        # â­ ãƒ‘ã‚¿ãƒ¼ãƒ³6: æ‹¬å¼§å†…ã®ç•¥èªã‚„æ³¨é‡ˆ
        # ä¾‹: "(NASA)", "(e.g.)", "(et al.)"
        pattern6 = r'\(([A-Z][A-Za-z\.]+)\)'
        matches6 = re.finditer(pattern6, text)
        for match in matches6:
            add_noun(match.group(1))
        
        logger.info(f"ğŸ” å›ºæœ‰åè©ä¿è­·: {len(proper_nouns)}å€‹ã‚’æ¤œå‡º")
        return proper_nouns
    
    def _protect_proper_nouns(self, text: str) -> Tuple[str, List[Tuple[str, str]]]:
        """
        â­ å›ºæœ‰åè©ã‚’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ä¿è­·
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            (ä¿è­·ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ, [(å…ƒã®å›ºæœ‰åè©, ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼), ...])
        """
        proper_nouns = self._extract_proper_nouns(text)
        protected_text = text
        
        # é•·ã„å›ºæœ‰åè©ã‹ã‚‰ç½®æ›ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        for noun, placeholder in sorted(proper_nouns, key=lambda x: len(x[0]), reverse=True):
            protected_text = protected_text.replace(noun, placeholder)
        
        if proper_nouns:
            logger.info(f"ğŸ”’ å›ºæœ‰åè©ä¿è­·: {len(proper_nouns)}å€‹ - {[n[0] for n in proper_nouns[:5]]}")
        
        return protected_text, proper_nouns
    
    def _restore_proper_nouns(self, text: str, proper_nouns: List[Tuple[str, str]]) -> str:
        """
        â­ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å…ƒã®å›ºæœ‰åè©ã«æˆ»ã™ï¼ˆå£Šã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚‚å¯¾å¿œï¼‰
        
        Args:
            text: ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            proper_nouns: [(å…ƒã®å›ºæœ‰åè©, ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼), ...]
            
        Returns:
            å›ºæœ‰åè©ãŒå¾©å…ƒã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        import re
        
        restored_text = text
        restored_count = 0
        
        for noun, placeholder in proper_nouns:
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç•ªå·ã‚’æŠ½å‡º (ä¾‹: __NOUN001__ â†’ 001)
            match = re.search(r'(\d{3})', placeholder)
            if not match:
                continue
            num = match.group(1)
            
            # ğŸ“Œ æ–°å½¢å¼ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ__NOUN###__ï¼‰
            broken_patterns = [
                placeholder,  # å®Œå…¨ä¸€è‡´ (__NOUN001__)
                rf'__NOUN\s*{num}__',  # ã‚¹ãƒšãƒ¼ã‚¹æ··å…¥ç‰ˆ
                rf'_+NOUN\s*{num}_+',  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢å¤‰å‹•ç‰ˆ
                rf'NOUN\s*{num}',  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢å‰Šé™¤ç‰ˆ
                rf'åè©\s*{num}',  # æ—¥æœ¬èªå¤‰æ›ç‰ˆ
                rf'ãƒã‚¦ãƒ³\s*{num}',  # ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ç‰ˆ
                
                # ğŸ”§ æ—§å½¢å¼ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆPROPERNOUNKEPTï¼‰ã‚‚å¿µã®ãŸã‚å¯¾å¿œ
                rf'PROPERN?O?\s*UNKEPT\s*{num}',
                rf'PRO\s*PERNO\s*UNKEPT\s*{num}',
                rf'ãƒ—ãƒ­PERNO\s*UNKEPT\s*{num}',
                rf'ãƒ—ãƒ­ãƒšãƒ«[ãƒŒãƒ]ãƒ³?ã‚±ãƒ—ãƒˆ\s*{num}',  # ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ç‰ˆ
                rf'ãƒ—ãƒ­ãƒšãƒ¼ãƒ«?[ãƒŒãƒ]ãƒ³?[ã‚¯ã‚±][ã‚§ã‚¨]?ãƒ—ãƒˆ\s*{num}',  # ã•ã‚‰ã«å£Šã‚ŒãŸç‰ˆ
                rf'PROPN\s*{num}',
                rf'PROPER\s*NOUN\s*KEPT\s*{num}',
                rf'å›ºæœ‰åè©\s*{num}',
                rf'[PĞ ]ROP[EĞ•]R[NĞPĞ ]?[OĞ]?U?N?K[EĞ•]PT\s*{num}',
                rf'[A-ZĞ-Ğ¯]{{2,}}\s*[UN]?KEPT\s*{num}',
            ]
            
            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã—ã¦å¾©å…ƒ
            found = False
            for pattern in broken_patterns:
                if isinstance(pattern, str) and pattern == placeholder:
                    # å®Œå…¨ä¸€è‡´ã®å ´åˆ
                    if pattern in restored_text:
                        restored_text = restored_text.replace(pattern, noun)
                        restored_count += 1
                        logger.info(f"ğŸ”§ å¾©å…ƒ: '{pattern}' â†’ {noun}")
                        found = True
                        break
                else:
                    # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ
                    matches = list(re.finditer(pattern, restored_text, re.IGNORECASE))
                    if matches:
                        for m in matches:
                            restored_text = restored_text.replace(m.group(0), noun)
                            restored_count += 1
                            logger.info(f"ğŸ”§ ä¿®å¾©: '{m.group(0)}' â†’ {noun}")
                        found = True
                        break
            
            if not found:
                logger.warning(f"âš ï¸ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æœªç™ºè¦‹: {placeholder} (å…ƒ: {noun})")
        
        if proper_nouns:
            logger.info(f"ğŸ”“ å›ºæœ‰åè©å¾©å…ƒ: {restored_count}/{len(proper_nouns)}å€‹")
        
        return restored_text
    
    def _post_process_japanese(self, text: str) -> str:
        """
        æ—¥æœ¬èªç¿»è¨³ã®å¾Œå‡¦ç†
        - å¥èª­ç‚¹ã®ä¿®æ­£
        - ä¸è‡ªç„¶ãªç¿»è¨³ã®ä¿®æ­£
        - ç¹°ã‚Šè¿”ã—ã®é™¤å»(æ…é‡ã«)
        """
        import re
        
        # â­â­â­ è‹±èªã¨æ—¥æœ¬èªã®å¥èª­ç‚¹æ··åœ¨ã‚’ä¿®æ­£
        # 1. ã¾ãšè‹±èªã®å¥èª­ç‚¹ã‚’å…¨ã¦æ—¥æœ¬èªã«çµ±ä¸€
        text = text.replace(',', 'ã€')
        text = text.replace('.', 'ã€‚')
        
        # 2. è‹±èªå¥èª­ç‚¹ã®æ®‹éª¸ã‚’å‰Šé™¤ï¼ˆã‚¹ãƒšãƒ¼ã‚¹+å¥èª­ç‚¹ã®çµ„ã¿åˆã‚ã›ï¼‰
        text = re.sub(r'\s*\.\s*', 'ã€‚', text)  # . â†’ ã€‚
        text = re.sub(r'\s*,\s*', 'ã€', text)   # , â†’ ã€
        
        # 3. æ··åœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
        text = text.replace('.ã€‚', 'ã€‚')  # .ã€‚ â†’ ã€‚
        text = text.replace('ã€‚.', 'ã€‚')  # ã€‚. â†’ ã€‚
        text = text.replace(',ã€', 'ã€')  # ,ã€ â†’ ã€
        text = text.replace('ã€,', 'ã€')  # ã€, â†’ ã€
        
        # 4. é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã‚’ä¿®æ­£
        text = re.sub(r'ã€‚{2,}', 'ã€‚', text)  # ã€‚ã€‚ â†’ ã€‚
        text = re.sub(r'ã€{2,}', 'ã€', text)  # ã€ã€ â†’ ã€
        
        # 5. æ–‡æœ«ã®å¥ç‚¹ã‚’æ­£è¦åŒ–
        text = re.sub(r'([^ã€‚])$', r'\1ã€‚', text)  # æ–‡æœ«ã«å¥ç‚¹ãŒãªã„å ´åˆã¯è¿½åŠ 
        text = re.sub(r'ã€‚+$', 'ã€‚', text)  # æ–‡æœ«ã®é€£ç¶šå¥ç‚¹ã‚’1ã¤ã«
        
        # 6. ã‚¹ãƒšãƒ¼ã‚¹ã®æ­£è¦åŒ–
        text = re.sub(r'\s{2,}', ' ', text)  # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
        text = re.sub(r'\s+([ã€ã€‚!?])', r'\1', text)  # å¥èª­ç‚¹ã®å‰ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
        text = re.sub(r'([ã€ã€‚])\s+', r'\1', text)  # å¥èª­ç‚¹ã®å¾Œã®è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
        
        # ä¸è‡ªç„¶ãªè¡¨ç¾ã®ä¿®æ­£(å®Œå…¨ä¸€è‡´ã®ã¿)
        replacements = {
            # é‡è¤‡è¡¨ç¾(å®Œå…¨ä¸€è‡´)
            'çµ±è¨ˆèª¿æŸ»ãªã©çµ±è¨ˆèª¿æŸ»ãªã©': 'çµ±è¨ˆèª¿æŸ»ãªã©',
            'èª¿æŸ»èª¿æŸ»': 'èª¿æŸ»',
            'ç ”ç©¶ç ”ç©¶': 'ç ”ç©¶',
            'ç›®çš„ç›®çš„': 'ç›®çš„',
            'å‹•æ©Ÿå‹•æ©Ÿ': 'å‹•æ©Ÿ',
            'è¦³å…‰è¦³å…‰': 'è¦³å…‰',
            
            # ä¸è‡ªç„¶ãªåŠ©è©ã®é€£ç¶š
            'ã®ã®': 'ã®',
            'ã‚’ã‚’': 'ã‚’',
            'ã«ã«': 'ã«',
            'ã¨ã¨': 'ã¨',
            'ã§ã§': 'ã§',
            'ãŒãŒ': 'ãŒ',
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # æ–‡é ­ãƒ»æ–‡æœ«ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
        text = text.strip()
        
        # T5ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸã€Œè¦ç´„: ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
        if text.startswith('è¦ç´„:') or text.startswith('è¦ç´„ :'):
            text = text.replace('è¦ç´„:', '').replace('è¦ç´„ :', '').strip()
        
        # ç©ºã®æ‹¬å¼§ã‚’å‰Šé™¤
        text = re.sub(r'[(]\s*[)]', '', text)
        text = re.sub(r'[(]\s*[)]', '', text)  # 2å›å®Ÿè¡Œã—ã¦é€£ç¶šæ‹¬å¼§ã‚‚é™¤å»
        
        # è¾æ›¸ã«ã‚ˆã‚‹ç½®æ›
        term_corrections = self._collect_term_corrections(source_lang="eng_Latn", target_lang="jpn_Jpan")
        for wrong, correct in term_corrections.items():
            text = text.replace(wrong, correct)
        
        # â­ å›ºæœ‰åè©ã®èª¤è¨³ã‚’ä¿®æ­£ï¼ˆé‡è¦ï¼ï¼‰
        # ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãŒèª¤è¨³ã—ã‚„ã™ã„å˜èªã‚’æ‰‹å‹•ä¿®æ­£
        common_mistranslations = {
            'ç†ŠçŒ«': 'ã‚¯ãƒ¼ã‚¬ãƒ¼',  # cougar â†’ ç†ŠçŒ«(ãƒ‘ãƒ³ãƒ€) ã®èª¤è¨³
            'ãƒ”ãƒ¥ãƒ¼ãƒ': 'ã‚¯ãƒ¼ã‚¬ãƒ¼',  # çµ±ä¸€ã®ãŸã‚
            'ãƒã‚¦ãƒ³ãƒ†ãƒ³ãƒ©ã‚¤ã‚ªãƒ³': 'ã‚¯ãƒ¼ã‚¬ãƒ¼',  # åŒç¾©èª
        }
        for wrong, correct in common_mistranslations.items():
            text = text.replace(wrong, correct)
        
        # â­ è‹±èªã®å›ºæœ‰åè©ã‚’æ—¥æœ¬èªã«å¤‰æ›
        english_to_japanese = {
            'cougar': 'ã‚¯ãƒ¼ã‚¬ãƒ¼',
            'Cougar': 'ã‚¯ãƒ¼ã‚¬ãƒ¼',
            'jaguar': 'ã‚¸ãƒ£ã‚¬ãƒ¼',
            'Jaguar': 'ã‚¸ãƒ£ã‚¬ãƒ¼',
            'panther': 'ãƒ‘ãƒ³ã‚µãƒ¼',
            'Panther': 'ãƒ‘ãƒ³ã‚µãƒ¼',
            'United States': 'ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½',
            'North American': 'åŒ—ã‚¢ãƒ¡ãƒªã‚«ã®',
            'Florida Panther': 'ãƒ•ãƒ­ãƒªãƒ€ãƒ‘ãƒ³ã‚µãƒ¼',
            'The Florida Panther': 'ãƒ•ãƒ­ãƒªãƒ€ãƒ‘ãƒ³ã‚µãƒ¼',
        }
        for eng, jpn in english_to_japanese.items():
            text = text.replace(eng, jpn)
        
        # â­ ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰Šé™¤ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        # 1. åŒã˜å˜èªãŒ3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹å ´åˆã€1å›ã«æ¸›ã‚‰ã™
        text = re.sub(r'(\w{1,3})\1{2,}', r'\1', text)
        
        # 2. ã‚«ã‚¿ã‚«ãƒŠã®ç¹°ã‚Šè¿”ã—ï¼ˆä¾‹: ãƒ”ãƒ¥ãƒ¼ãƒãƒ”ãƒ¥ãƒ¼ãƒãƒ”ãƒ¥ãƒ¼ãƒ â†’ ãƒ”ãƒ¥ãƒ¼ãƒï¼‰
        text = re.sub(r'([ã‚¡-ãƒ´ãƒ¼]{2,})\1{2,}', r'\1', text)
        
        # 3. åŒã˜ãƒ•ãƒ¬ãƒ¼ã‚ºã®ç¹°ã‚Šè¿”ã—ï¼ˆå¥èª­ç‚¹åŒºåˆ‡ã‚Šï¼‰
        # ä¾‹: ã€ŒåŒ—æ¥µã€åŒ—æ¥µã€åŒ—æ¥µã€â†’ã€ŒåŒ—æ¥µã€
        text = re.sub(r'([^ã€ã€‚]{3,})[ã€ã€‚]\s*\1[ã€ã€‚]\s*\1', r'\1', text)
        
        # 4. æ–‡ã®ç¹°ã‚Šè¿”ã—ï¼ˆ2å›ä»¥ä¸Šï¼‰
        # ä¾‹: ã€Œã“ã®å‹•ç‰©ã¯...ã€‚ã“ã®å‹•ç‰©ã¯...ã€‚ã€â†’ã€Œã“ã®å‹•ç‰©ã¯...ã€‚ã€
        sentences = text.split('ã€‚')
        unique_sentences = []
        seen = set()
        for sent in sentences:
            sent_clean = sent.strip()
            if sent_clean and sent_clean not in seen:
                unique_sentences.append(sent_clean)
                seen.add(sent_clean)
        text = 'ã€‚'.join(unique_sentences)
        if text and not text.endswith('ã€‚'):
            text += 'ã€‚'

        return text
    
    def _calculate_quality_metrics(self, original_text: str, summary_text: str, execution_time: float, model_name: str = "mBART-50") -> Dict[str, Any]:
        """
        AIè¦ç´„å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ï¼ˆå°±æ´»ã‚¢ãƒ”ãƒ¼ãƒ«ç”¨ï¼‰
        
        Args:
            original_text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            summary_text: è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
            execution_time: å‡¦ç†æ™‚é–“
            model_name: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾æ›¸
        """
        import re
        from collections import Counter
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        original_length = len(original_text)
        summary_length = len(summary_text)
        compression_ratio = (1 - summary_length / original_length) * 100 if original_length > 0 else 0
        
        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…ç‡åˆ†æ
        def extract_keywords(text, min_length=2):
            """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆTF-IDFé¢¨ï¼‰"""
            # åè©ãƒ»å‹•è©ã£ã½ã„å˜èªã‚’æŠ½å‡ºï¼ˆã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã‚’å«ã‚€2æ–‡å­—ä»¥ä¸Šï¼‰
            words = re.findall(r'[ã‚¡-ãƒ´ãƒ¼ä¸€-é¾¥]{2,}', text)
            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
            stopwords = {'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'ã“ã‚Œ', 'ãã‚Œ', 'ã©ã‚Œ', 'ã“ã“', 'ãã“', 'ã©ã“', 
                        'ãªã©', 'ã¨ã—ã¦', 'ã«ã¤ã„ã¦', 'ã«ã‚ˆã‚‹', 'ã«ãŠã„ã¦', 'ã¨ã„ã†'}
            words = [w for w in words if w not in stopwords and len(w) >= min_length]
            return Counter(words)
        
        original_keywords = extract_keywords(original_text)
        summary_keywords = extract_keywords(summary_text)
        
        # å…ƒãƒ†ã‚­ã‚¹ãƒˆã®ä¸Šä½20ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã©ã‚Œã ã‘è¦ç´„ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹
        top_keywords = [word for word, _ in original_keywords.most_common(20)]
        coverage_count = sum(1 for word in top_keywords if word in summary_text)
        keyword_coverage = (coverage_count / len(top_keywords) * 100) if top_keywords else 0
        
        # 3. æ–‡ç« è‡ªç„¶åº¦ï¼ˆç°¡æ˜“è©•ä¾¡ï¼‰
        # åŠ©è©ã®é©åˆ‡ãªä½¿ç”¨ã€æ–‡ã®é•·ã•ãƒãƒ©ãƒ³ã‚¹ãªã©ã§åˆ¤å®š
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', summary_text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if sentences:
            avg_sentence_length = sum(len(s) for s in sentences) / len(sentences)
            # 20-40æ–‡å­—ãŒé©åˆ‡ãªæ–‡ã®é•·ã•ã¨ä»®å®š
            length_score = 100 - abs(avg_sentence_length - 30) * 2
            length_score = max(0, min(100, length_score))
            
            # åŠ©è©ã®ä½¿ç”¨é »åº¦ï¼ˆé©åˆ‡ãªæ–‡ç« ã¯7-15%ç¨‹åº¦ï¼‰
            particles = len(re.findall(r'[ã¯ãŒã‚’ã«ã¸ã¨ã§ã‚„]', summary_text))
            particle_ratio = particles / summary_length * 100 if summary_length > 0 else 0
            particle_score = 100 - abs(particle_ratio - 10) * 5
            particle_score = max(0, min(100, particle_score))
            
            # ç·åˆè‡ªç„¶åº¦
            naturalness = (length_score * 0.6 + particle_score * 0.4)
        else:
            naturalness = 50  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # 4. ç·åˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…ç‡50%ã€è‡ªç„¶åº¦30%ã€åœ§ç¸®ç‡20%ã®é‡ã¿ä»˜ã‘
        confidence_score = (
            keyword_coverage * 0.5 +
            naturalness * 0.3 +
            min(compression_ratio, 100) * 0.2
        )
        
        # 5. å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if confidence_score >= 90:
            quality_level = "æœ€é«˜å“è³ª"
            quality_color = "success"
        elif confidence_score >= 75:
            quality_level = "é«˜å“è³ª"
            quality_color = "info"
        elif confidence_score >= 60:
            quality_level = "è‰¯å¥½"
            quality_color = "primary"
        elif confidence_score >= 45:
            quality_level = "æ¨™æº–"
            quality_color = "warning"
        else:
            quality_level = "è¦æ”¹å–„"
            quality_color = "danger"
        
        # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«
        chars_per_sec = original_length / execution_time if execution_time > 0 else 0
        if chars_per_sec > 200:
            performance_level = "è¶…é«˜é€Ÿ"
            performance_icon = "âš¡âš¡âš¡"
        elif chars_per_sec > 150:
            performance_level = "é«˜é€Ÿ"
            performance_icon = "âš¡âš¡"
        elif chars_per_sec > 100:
            performance_level = "æ¨™æº–"
            performance_icon = "âš¡"
        else:
            performance_level = "å‡¦ç†ä¸­"
            performance_icon = "ğŸ¢"
        
        return {
            "confidence_score": round(confidence_score, 1),
            "keyword_coverage": round(keyword_coverage, 1),
            "naturalness": round(naturalness, 1),
            "compression_ratio": round(compression_ratio, 1),
            "quality_level": quality_level,
            "quality_color": quality_color,
            "performance": {
                "chars_per_sec": round(chars_per_sec, 1),
                "performance_level": performance_level,
                "performance_icon": performance_icon
            },
            "statistics": {
                "original_length": original_length,
                "summary_length": summary_length,
                "execution_time": round(execution_time, 2),
                "sentence_count": len(sentences),
                "avg_sentence_length": round(sum(len(s) for s in sentences) / len(sentences), 1) if sentences else 0
            },
            "top_keywords": [word for word, _ in summary_keywords.most_common(5)],
            "model_info": {
                "name": model_name,
                "type": "Transformer (mBART/DistilBART)",
                "optimization": "CPUæœ€é©åŒ– (torch.no_grad + beam=2)"
            }
        }
    
    def _convert_to_bullet_points(self, text: str) -> str:
        """
        æ®µè½å‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ç®‡æ¡æ›¸ãå½¢å¼ã«å¤‰æ›
        
        Args:
            text: è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ç®‡æ¡æ›¸ãå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        import re
        
        # æ–‡å˜ä½ã§åˆ†å‰²ï¼ˆæ”¹è¡Œã¾ãŸã¯å¥ç‚¹ã§åˆ†å‰²ï¼‰
        # æ”¹è¡Œã§åˆ†ã‘ã¦ã‹ã‚‰ã€ã•ã‚‰ã«å¥ç‚¹ã§åˆ†å‰²
        lines = text.split('\n')
        sentences = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å¥ç‚¹ã§åˆ†å‰²
            parts = re.split(r'([ã€‚ï¼ï¼Ÿ])', line)
            current = ""
            
            for i, part in enumerate(parts):
                if part in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
                    if current:
                        sentences.append(current + part)
                        current = ""
                else:
                    current += part
            
            # æ®‹ã‚ŠãŒã‚ã‚Œã°è¿½åŠ 
            if current.strip():
                # å¥ç‚¹ãŒãªã„å ´åˆã¯è¿½åŠ 
                if not current.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                    sentences.append(current.strip() + 'ã€‚')
                else:
                    sentences.append(current.strip())
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã®ã¿æŠ½å‡º
        unique_points = []
        seen = set()
        
        for sent in sentences:
            # çŸ­ã™ãã‚‹æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(sent) < 10:
                continue
            
            # é¡ä¼¼æ–‡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¥èª­ç‚¹ã¨ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ã—ã¦æ¯”è¼ƒï¼‰
            normalized = re.sub(r'[ã€ã€‚ï¼ï¼Ÿ\s]', '', sent)
            if normalized not in seen and normalized:
                seen.add(normalized)
                unique_points.append(sent)
        
        # æœ€å¤§5ãƒã‚¤ãƒ³ãƒˆã«åˆ¶é™
        main_points = unique_points[:5]
        
        # ç®‡æ¡æ›¸ãå½¢å¼ã«æ•´å½¢
        if not main_points:
            return text  # å¤‰æ›å¤±æ•—æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
        
        bullet_text = "ã€ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã€‘\n\n"
        for i, point in enumerate(main_points):
            # â­ å¥èª­ç‚¹ã‚’æ­£è¦åŒ–ï¼ˆè‹±èªå¥èª­ç‚¹ã®æ®‹éª¸ã‚’å‰Šé™¤ï¼‰
            point = point.replace('.', 'ã€‚').replace(',', 'ã€')
            point = point.replace('.ã€‚', 'ã€‚').replace(',ã€', 'ã€')
            point = re.sub(r'ã€‚{2,}', 'ã€‚', point)  # é€£ç¶šå¥ç‚¹å‰Šé™¤
            point = re.sub(r'ã€{2,}', 'ã€', point)  # é€£ç¶šèª­ç‚¹å‰Šé™¤
            
            # ã€Œã§ã™ãƒ»ã¾ã™ã€ã‚’çµ±ä¸€
            point = point.replace('ã§ã‚ã‚‹ã€‚', 'ã§ã™ã€‚')
            point = point.replace('ã§ã‚ã£ãŸã€‚', 'ã§ã—ãŸã€‚')
            point = point.replace('ã§ã‚ã‚Šã€', 'ã§ã€')
            
            # æ–‡æœ«ã«å¥ç‚¹ãŒãªã„å ´åˆã¯è¿½åŠ 
            if not point.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                point += 'ã€‚'
            
            # ç®‡æ¡æ›¸ãè¨˜å·ã‚’è¿½åŠ ï¼ˆå„ãƒã‚¤ãƒ³ãƒˆã®å¾Œã«ç©ºè¡Œã‚’è¿½åŠ ï¼‰
            bullet_text += f"â€¢ {point}\n"
            
            # æœ€å¾Œã®ãƒã‚¤ãƒ³ãƒˆä»¥å¤–ã¯ç©ºè¡Œã‚’è¿½åŠ 
            if i < len(main_points) - 1:
                bullet_text += "\n"
        
        logger.info(f"ğŸ“‹ ç®‡æ¡æ›¸ãå¤‰æ›: {len(sentences)}æ–‡ â†’ {len(main_points)}ãƒã‚¤ãƒ³ãƒˆ")
        
        return bullet_text.strip()
    
    def _calculate_quality_metrics(self, original_text: str, summary_text: str, execution_time: float, model_name: str) -> Dict[str, Any]:
        """
        AIè¦ç´„ã®å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ï¼ˆå°±æ´»ã‚¢ãƒ”ãƒ¼ãƒ«ç”¨ï¼‰
        
        Args:
            original_text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            summary_text: è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
            execution_time: å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
            model_name: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸
        """
        import re
        from collections import Counter
        
        # 1. åœ§ç¸®ç‡åˆ†æ
        original_length = len(original_text)
        summary_length = len(summary_text)
        compression_ratio = (1 - summary_length / original_length) * 100 if original_length > 0 else 0
        
        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…ç‡åˆ†æ
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆåè©ã€å°‚é–€ç”¨èªï¼‰
        def extract_keywords(text: str) -> Counter:
            # ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
            katakana_words = re.findall(r'[ã‚¡-ãƒ´ãƒ¼]{3,}', text)
            # æ¼¢å­—èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
            kanji_words = re.findall(r'[ä¸€-é¾¥]{2,}', text)
            # è‹±å˜èªï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰
            english_words = re.findall(r'[A-Za-z]{3,}', text)
            
            all_keywords = katakana_words + kanji_words + english_words
            return Counter(all_keywords)
        
        original_keywords = extract_keywords(original_text)
        summary_keywords = extract_keywords(summary_text)
        
        # â­â­â­ æ”¹å–„: æƒ…å ±ç¶²ç¾…ç‡ã®è¨ˆç®—æ–¹æ³•ã‚’å¤‰æ›´
        # ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘ã§ãªãã€å…¨ä½“çš„ãªæƒ…å ±ä¿æŒç‡ã‚’è¨ˆç®—
        top_keywords = [word for word, _ in original_keywords.most_common(20)]  # 10â†’20ã«å¢—åŠ 
        
        # æ–¹æ³•1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’è€ƒæ…®
        coverage_score = 0
        for keyword in top_keywords:
            if keyword in summary_text:
                # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®é‡è¦åº¦ï¼ˆå‡ºç¾å›æ•°ï¼‰ã‚’è€ƒæ…®
                original_count = original_keywords[keyword]
                summary_count = summary_text.count(keyword)
                # æœ€ä½1å›å‡ºç¾ã—ã¦ã„ã‚Œã°åŸºæœ¬ãƒã‚¤ãƒ³ãƒˆã€è¤‡æ•°å›ãªã‚‰ãƒœãƒ¼ãƒŠã‚¹
                coverage_score += min(summary_count / original_count, 1.0) * 100 / len(top_keywords)
        
        keyword_coverage = min(coverage_score, 100)  # 100%ã‚’è¶…ãˆãªã„ã‚ˆã†ã«
        
        # æ–¹æ³•2: æ–‡ã®æ„å‘³çš„ãªç¶²ç¾…ç‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        original_sentences = re.split(r'[ã€‚.!?]', original_text)
        original_sentences = [s.strip() for s in original_sentences if len(s.strip()) > 10]
        
        # è¦ç´„ã«å«ã¾ã‚Œã‚‹å…ƒã®æ–‡ã®æ–­ç‰‡ã‚’è¨ˆç®—
        sentence_coverage = 0
        for orig_sent in original_sentences[:30]:  # æœ€åˆã®30æ–‡ã‚’å¯¾è±¡
            # 5æ–‡å­—ä»¥ä¸Šã®éƒ¨åˆ†æ–‡å­—åˆ—ãŒè¦ç´„ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹
            words_in_orig = re.findall(r'[ä¸€-é¾¥ã‚¡-ãƒ´ãƒ¼a-zA-Z]{3,}', orig_sent)
            if words_in_orig:
                matched = sum(1 for word in words_in_orig if word in summary_text)
                if matched > 0:
                    sentence_coverage += (matched / len(words_in_orig))
        
        sentence_coverage_rate = min((sentence_coverage / min(len(original_sentences), 30)) * 100, 100) if original_sentences else 0
        
        # æœ€çµ‚çš„ãªæƒ…å ±ç¶²ç¾…ç‡: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…70% + æ–‡ç« ç¶²ç¾…30%
        final_coverage = keyword_coverage * 0.7 + sentence_coverage_rate * 0.3
        
        # 3. æ–‡ç« è‡ªç„¶åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # å¥ç‚¹ã®æ•°ã¨æ–‡å­—æ•°ã®æ¯”ç‡ã§åˆ¤å®š
        sentence_count = summary_text.count('ã€‚') + summary_text.count('. ')
        avg_sentence_length = summary_length / sentence_count if sentence_count > 0 else 0
        
        # é©åˆ‡ãªæ–‡ã®é•·ã•: 30-80æ–‡å­—
        if 30 <= avg_sentence_length <= 80:
            naturalness = 95
        elif 20 <= avg_sentence_length <= 100:
            naturalness = 85
        else:
            naturalness = 75
        
        # 4. ç·åˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        # é‡ã¿ä»˜ã‘: æƒ…å ±ç¶²ç¾…ç‡40% + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…ç‡20% + è‡ªç„¶åº¦30% + åœ§ç¸®å“è³ª10%
        compression_quality = 100 if 90 <= compression_ratio <= 98 else 80 if 80 <= compression_ratio <= 99 else 60
        confidence_score = (
            final_coverage * 0.4 +      # â­ æƒ…å ±ç¶²ç¾…ç‡ï¼ˆæ–°è¦ï¼‰
            keyword_coverage * 0.2 +     # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¶²ç¾…ç‡
            naturalness * 0.3 +          # æ–‡ç« è‡ªç„¶åº¦
            compression_quality * 0.1    # åœ§ç¸®å“è³ª
        )
        
        # 5. å“è³ªè©•ä¾¡ãƒ¬ãƒ™ãƒ«
        if confidence_score >= 90:
            quality_level = "éå¸¸ã«é«˜å“è³ª"
            quality_color = "success"
        elif confidence_score >= 80:
            quality_level = "é«˜å“è³ª"
            quality_color = "success"
        elif confidence_score >= 70:
            quality_level = "è‰¯å¥½"
            quality_color = "info"
        elif confidence_score >= 60:
            quality_level = "æ¨™æº–"
            quality_color = "warning"
        else:
            quality_level = "è¦æ”¹å–„"
            quality_color = "danger"
        
        # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        chars_per_second = original_length / execution_time if execution_time > 0 else 0
        
        if chars_per_second > 200:
            performance_level = "è¶…é«˜é€Ÿ"
            performance_icon = "âš¡âš¡âš¡"
        elif chars_per_second > 150:
            performance_level = "é«˜é€Ÿ"
            performance_icon = "âš¡âš¡"
        elif chars_per_second > 100:
            performance_level = "æ¨™æº–"
            performance_icon = "âš¡"
        else:
            performance_level = "å‡¦ç†ä¸­"
            performance_icon = "ğŸŒ"
        
        return {
            "confidence_score": round(confidence_score, 1),
            "information_coverage": round(final_coverage, 1),  # â­ æƒ…å ±ç¶²ç¾…ç‡ã‚’è¿½åŠ 
            "keyword_coverage": round(keyword_coverage, 1),
            "naturalness": round(naturalness, 1),
            "compression_ratio": round(compression_ratio, 1),
            "compression_quality": compression_quality,
            "quality_level": quality_level,
            "quality_color": quality_color,
            "performance": {
                "chars_per_second": round(chars_per_second, 1),
                "performance_level": performance_level,
                "performance_icon": performance_icon
            },
            "statistics": {
                "original_length": original_length,
                "summary_length": summary_length,
                "execution_time": round(execution_time, 2),
                "sentence_count": sentence_count,
                "avg_sentence_length": round(avg_sentence_length, 1)
            },
            "top_keywords": [word for word, _ in summary_keywords.most_common(5)],
            "model_info": {
                "name": model_name,
                "type": "Transformer (mBART/DistilBART)",
                "optimization": "CPUæœ€é©åŒ– (torch.no_grad + beam=2)"
            }
        }
    
    def _apply_style(self, text: str, style: str) -> str:
        """
        è¦ç´„ã«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨
        
        Args:
            text: è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
            style: 'bullets', 'academic', 'business', 'casual', 'balanced'
            
        Returns:
            ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if style in ('bullet', 'bullets'):
            # ç®‡æ¡æ›¸ãã‚¹ã‚¿ã‚¤ãƒ«: ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æ˜ç¢ºã«
            return self._convert_to_bullet_points(text)
        
        elif style == 'academic':
            # å­¦è¡“çš„ã‚¹ã‚¿ã‚¤ãƒ«: æ•¬ä½“ã€å°‚é–€ç”¨èªã€å®¢è¦³çš„è¡¨ç¾
            replacements = {
                'ã§ã™ã€‚': 'ã§ã‚ã‚‹ã€‚',
                'ã¾ã™ã€‚': 'ã‚‹ã€‚',
                'ã§ã—ãŸã€‚': 'ã§ã‚ã£ãŸã€‚',
                'ã¾ã—ãŸã€‚': 'ãŸã€‚',
                'æ€ã„ã¾ã™': 'è€ƒãˆã‚‰ã‚Œã‚‹',
                'æ€ã‚ã‚Œã¾ã™': 'è€ƒãˆã‚‰ã‚Œã‚‹',
                'ã§ãã¾ã™': 'ã§ãã‚‹',
                'ã„ã¾ã™': 'ã„ã‚‹',
                'ã‚ã‚Šã¾ã™': 'ã‚ã‚‹',
                'ã€œã¨è¨€ãˆã¾ã™': 'ã€œã¨è¨€ãˆã‚‹',
                'ã€œãŒåˆ†ã‹ã‚Šã¾ã™': 'ã€œãŒæ˜ã‚‰ã‹ã§ã‚ã‚‹',
            }
            for old, new in replacements.items():
                text = text.replace(old, new)
            logger.info("ğŸ“ å­¦è¡“çš„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨")
            
        elif style == 'business':
            # ãƒ“ã‚¸ãƒã‚¹ã‚¹ã‚¿ã‚¤ãƒ«: ç°¡æ½”ã€è¦ç‚¹æ˜ç¢ºã€ä¸å¯§èª
            replacements = {
                'ã€œã¨æ€ã„ã¾ã™': 'ã€œã¨è€ƒãˆã¾ã™',
                'ã€œã ã¨æ€ã‚ã‚Œã¾ã™': 'ã€œã¨èªè­˜ã—ã¦ãŠã‚Šã¾ã™',
                'ã§ãã¾ã™': 'å¯èƒ½ã§ã™',
                'ã—ã¾ã™': 'ã„ãŸã—ã¾ã™',
                'è‰¯ã„': 'åŠ¹æœçš„ãª',
                'æ‚ªã„': 'èª²é¡Œã®ã‚ã‚‹',
                'å¤šã„': 'å¤šæ•°ã®',
                'å°‘ãªã„': 'é™å®šçš„ãª',
            }
            for old, new in replacements.items():
                text = text.replace(old, new)
            
            # ç®‡æ¡æ›¸ãé¢¨ã«ãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿
            text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*', r'\1\n', text)
            logger.info("ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨")
            
        elif style == 'casual':
            # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«: ãã ã‘ãŸè¡¨ç¾ã€èª­ã¿ã‚„ã™ã•é‡è¦–
            replacements = {
                'ã§ã‚ã‚‹ã€‚': 'ã§ã™ã€‚',
                'ã§ã‚ã£ãŸã€‚': 'ã§ã—ãŸã€‚',
                'ã€œã¨è€ƒãˆã‚‰ã‚Œã‚‹': 'ã€œã¨æ€ã‚ã‚Œã¾ã™',
                'æ˜ã‚‰ã‹ã§ã‚ã‚‹': 'åˆ†ã‹ã‚Šã¾ã™',
                'ç¤ºå”†ã—ã¦ã„ã‚‹': 'ç¤ºã—ã¦ã„ã¾ã™',
                'é‡è¦ã§ã‚ã‚‹': 'å¤§äº‹ã§ã™',
                'å¿…è¦ã§ã‚ã‚‹': 'å¿…è¦ã§ã™',
                'å¯èƒ½ã§ã‚ã‚‹': 'ã§ãã¾ã™',
            }
            for old, new in replacements.items():
                text = text.replace(old, new)
            
            # é›£ã—ã„æ¼¢å­—ã‚’å¹³ä»®åã«
            text = text.replace('æ•…ã«', 'ãªã®ã§')
            text = text.replace('å¾“ã£ã¦', 'ã—ãŸãŒã£ã¦')
            text = text.replace('å³ã¡', 'ã¤ã¾ã‚Š')
            logger.info("ğŸ˜Š ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨")
            
        else:  # balanced (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
            # ãƒãƒ©ãƒ³ã‚¹å‹: èª­ã¿ã‚„ã™ãã€ã‹ã¤æ­£ç¢º
            logger.info("âš–ï¸ ãƒãƒ©ãƒ³ã‚¹å‹ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰")
        
        return text
    
    def _translate_text(self, text: str, source_lang: str, target_lang: str, protect_nouns: bool = True) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç¿»è¨³ï¼ˆå›ºæœ‰åè©ä¿è­·å¯¾å¿œï¼‰
        
        Args:
            text: ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            source_lang: å…¥åŠ›è¨€èªã‚³ãƒ¼ãƒ‰ (ä¾‹: 'eng_Latn')
            target_lang: å‡ºåŠ›è¨€èªã‚³ãƒ¼ãƒ‰ (ä¾‹: 'jpn_Jpan')
            protect_nouns: â­ å›ºæœ‰åè©ã‚’ä¿è­·ã™ã‚‹ã‹
        
        Returns:
            ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # â­ å›ºæœ‰åè©ä¿è­·
        proper_nouns = []
        original_text = text
        if protect_nouns and source_lang.startswith('eng'):
            text, proper_nouns = self._protect_proper_nouns(text)
        
        translator = self._get_translation_pipeline()
        
        if isinstance(translator, dict) and 'tokenizer' in translator:
            tokenizer = translator['tokenizer']
            model = translator['model']
            
            # è¨€èªã‚³ãƒ¼ãƒ‰ã‚’è¨­å®š
            tokenizer.src_lang = source_lang
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True)
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            if self.device >= 0:
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã‚’å¼·åˆ¶
            forced_bos_token_id = tokenizer.lang_code_to_id[target_lang]
            
            # âš¡ ç¿»è¨³ã®max_new_tokensè¨­å®šï¼ˆå…¥åŠ›ã‚’å«ã¾ãªã„ç´”ç²‹ãªå‡ºåŠ›é•·ï¼‰
            # æ—¥æœ¬èªã¸ã®ç¿»è¨³ã¯è‹±èªã®2å€ç¨‹åº¦ã®é•·ã•ã«ãªã‚‹ãŸã‚ã€ååˆ†ãªä½™è£•ã‚’æŒãŸã›ã‚‹
            if target_lang == 'jpn_Jpan':
                # è‹±èªâ†’æ—¥æœ¬èª: å…¥åŠ›ã®2.5å€ï¼ˆæœ€å°256ã€æœ€å¤§1024ï¼‰
                max_new_tokens = max(256, min(1024, len(text) * 2))
            else:
                # ãã®ä»–ã®è¨€èª: å…¥åŠ›+150æ–‡å­—ï¼ˆæœ€å°256ã€æœ€å¤§1024ï¼‰
                max_new_tokens = max(256, min(1024, len(text) + 150))
            
            logger.info(f"ğŸ“ ç¿»è¨³è¨­å®š: max_new_tokens={max_new_tokens} (å…¥åŠ›{len(text)}æ–‡å­—)")
            
            outputs = model.generate(
                **inputs,
                forced_bos_token_id=forced_bos_token_id,
                max_new_tokens=max_new_tokens,  # â­ ç´”ç²‹ãªå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                min_new_tokens=max(50, max_new_tokens // 3),  # â­ æœ€å°ç”Ÿæˆé•·ã‚’è¨­å®š
                num_beams=2,  # â­ ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§å“è³ªå‘ä¸Š
                length_penalty=1.0,  # â­ é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ãªã—
                no_repeat_ngram_size=4,  # â­ 3â†’4: ç¹°ã‚Šè¿”ã—ã‚’ã‚ˆã‚Šå¼·ãé˜²æ­¢
                repetition_penalty=1.2,  # â­ è¿½åŠ : ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
                early_stopping=False,  # â­ æ—©æœŸåœæ­¢ã‚’ç„¡åŠ¹åŒ–
                do_sample=False  # â­ æ±ºå®šè«–çš„ç”Ÿæˆ
            )
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ğŸ” ãƒ‡ãƒãƒƒã‚°: ç¿»è¨³ç›´å¾Œã®çµæœã‚’ç¢ºèª
            logger.info(f"ğŸŒ ç¿»è¨³ç›´å¾Œ: {len(result)}æ–‡å­— - {result[:200]}")
            
            # â­ å›ºæœ‰åè©ã‚’å¾©å…ƒ
            if protect_nouns and proper_nouns:
                result = self._restore_proper_nouns(result, proper_nouns)
                logger.info(f"ğŸ”“ å›ºæœ‰åè©å¾©å…ƒå¾Œ: {len(result)}æ–‡å­— - {result[:200]}")
            
            # æ—¥æœ¬èªã®å ´åˆã€å¾Œå‡¦ç†
            if target_lang == 'jpn_Jpan':
                result = self._post_process_japanese(result)
                logger.info(f"ğŸ§¹ å¾Œå‡¦ç†å¾Œ: {len(result)}æ–‡å­— - {result[:200]}")
            
            return result
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯(pipelineä½¿ç”¨)
            trans_result = translator(text, max_length=512)
            result = text
            if isinstance(trans_result, list) and len(trans_result) > 0:
                if 'translation_text' in trans_result[0]:
                    result = trans_result[0]['translation_text']
                elif 'generated_text' in trans_result[0]:
                    result = trans_result[0]['generated_text']
            
            # â­ å›ºæœ‰åè©ã‚’å¾©å…ƒ
            if protect_nouns and proper_nouns:
                result = self._restore_proper_nouns(result, proper_nouns)
            
            # æ—¥æœ¬èªã®å ´åˆã€å¾Œå‡¦ç†
            if target_lang == 'jpn_Jpan':
                result = self._post_process_japanese(result)
            
            return result
    
    def _detect_language(self, text: str) -> bool:
        """è¨€èªåˆ¤å®š: æ—¥æœ¬èªãªã‚‰Trueã€è‹±èªãªã‚‰False"""
        japanese_chars = sum(1 for c in text[:1000] if ord(c) > 0x3000)
        return japanese_chars > 50
    
    def _chunk_text(self, text: str, max_length: int = 1024) -> list:
        """é•·æ–‡ã‚’åˆ†å‰²"""
        sentences = text.replace('ã€‚', 'ã€‚\n').replace('. ', '.\n').split('\n')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < max_length:
                current_chunk += sentence + " "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + " "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def summarize(self, text: str, summary_mode: str = 'short', source_lang: str = 'auto', target_lang: str = 'jpn_Jpan', style: str = 'balanced') -> HFResponse:
        """
        ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„(å¤šè¨€èªç¿»è¨³è¦ç´„å¯¾å¿œ + ã‚¹ã‚¿ã‚¤ãƒ«é¸æŠ)
        
        Args:
            text: è¦ç´„ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            summary_mode: 'short' (200-400å­—) ã¾ãŸã¯ 'long' (800-1000å­—)
            source_lang: å…¥åŠ›è¨€èª ('auto'ã§è‡ªå‹•åˆ¤å®šã€ã¾ãŸã¯NLLBè¨€èªã‚³ãƒ¼ãƒ‰)
            target_lang: å‡ºåŠ›è¨€èª (NLLBè¨€èªã‚³ãƒ¼ãƒ‰ã€ä¾‹: 'jpn_Jpan', 'eng_Latn')
            style: è¦ç´„ã‚¹ã‚¿ã‚¤ãƒ« ('academic', 'business', 'casual', 'balanced')
        """
        if not self.available:
            return HFResponse(
                success=False,
                result="",
                model_used="unavailable",
                error="Hugging Face TransformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            )
        
        try:
            import time
            start_time = time.time()
            
            # â­ è¨€èªåˆ¤å®šã‚’æ”¹å–„
            if source_lang == 'auto':
                # æ–‡å­—ãƒã‚§ãƒƒã‚¯ã§åˆ¤å®š
                japanese_chars = sum(1 for c in text[:1000] if 0x3040 <= ord(c) <= 0x30FF or 0x4E00 <= ord(c) <= 0x9FFF)
                chinese_chars = sum(1 for c in text[:1000] if 0x4E00 <= ord(c) <= 0x9FFF)
                korean_chars = sum(1 for c in text[:1000] if 0xAC00 <= ord(c) <= 0xD7AF)
                arabic_chars = sum(1 for c in text[:1000] if 0x0600 <= ord(c) <= 0x06FF)
                cyrillic_chars = sum(1 for c in text[:1000] if 0x0400 <= ord(c) <= 0x04FF)
                
                if japanese_chars > 50:
                    detected_lang = 'jpn_Jpan'
                elif chinese_chars > 30 and japanese_chars < 10:
                    detected_lang = 'zho_Hans'  # ç°¡ä½“ä¸­å›½èª
                elif korean_chars > 30:
                    detected_lang = 'kor_Hang'
                elif arabic_chars > 30:
                    detected_lang = 'arb_Arab'
                elif cyrillic_chars > 30:
                    detected_lang = 'rus_Cyrl'
                else:
                    # â­ ãã®ä»–ã®è¨€èªï¼ˆã‚¹ãƒ¯ãƒ’ãƒªèªãªã©ï¼‰ã¯è‹±èªæ‰±ã„
                    detected_lang = 'eng_Latn'
                    logger.info(f"ğŸŒ ãã®ä»–ã®è¨€èªã‚’è‹±èªã¨ã—ã¦å‡¦ç†ã—ã¾ã™")
            else:
                detected_lang = source_lang
            
            is_japanese = (detected_lang == 'jpn_Jpan')
            
            logger.info(f"ğŸ“ å…¥åŠ›è¨€èª: {detected_lang}")
            logger.info(f"ğŸ¯ å‡ºåŠ›è¨€èª: {target_lang}")
            logger.info(f"ğŸ“ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text)} æ–‡å­—")
            
            # åŒã˜è¨€èªã®å ´åˆã¯ç¿»è¨³ä¸è¦
            needs_translation = (detected_lang != target_lang)
            
            # â­ æ—¥æœ¬èªè¦ç´„ã®å ´åˆã¯è‹±èªã‚’çµŒç”±ã™ã‚‹
            translate_via_english = False
            if needs_translation and detected_lang == 'jpn_Jpan' and target_lang != 'eng_Latn':
                translate_via_english = True
                logger.info(f"ğŸ”„ æ—¥æœ¬èªâ†’{target_lang}: è‹±èªçµŒç”±ã§ç¿»è¨³ã—ã¾ã™")
            
            # âš¡ ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ­ã‚¸ãƒƒã‚¯
            use_japanese_model = False
            use_english_summarization = False
            
            # ğŸ”§ é‡è¦: mBARTã¯ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ãªã®ã§è¦ç´„ã«ã¯ä¸å‘ã
            # â†’ è¨€èªã”ã¨ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            
            if detected_lang == 'jpn_Jpan' and target_lang == 'jpn_Jpan':
                # æ—¥æœ¬èªâ†’æ—¥æœ¬èª: mBARTä½¿ç”¨
                use_japanese_model = True
                logger.info("ï¿½ æ—¥æœ¬èªâ†’æ—¥æœ¬èª: mBARTã§è¦ç´„")
            elif detected_lang == 'jpn_Jpan' and target_lang != 'jpn_Jpan':
                # æ—¥æœ¬èªâ†’ä»–è¨€èª: è‹±èªçµŒç”±ï¼ˆmBARTã¯è¦ç´„ä¸å¯ï¼‰
                use_english_summarization = True
                logger.info(f"ğŸ”„ æ—¥æœ¬èªâ†’{target_lang}: è‹±èªè¦ç´„+ç¿»è¨³")
            elif detected_lang != 'jpn_Jpan' and target_lang == 'jpn_Jpan':
                # ä»–è¨€èªâ†’æ—¥æœ¬èª: è‹±èªè¦ç´„+æ—¥æœ¬èªç¿»è¨³ï¼ˆé«˜é€Ÿç‰ˆï¼‰
                use_english_summarization = True
                logger.info(f"ğŸš€ {detected_lang}â†’æ—¥æœ¬èª: è‹±èªè¦ç´„(çŸ­æ–‡)+ç¿»è¨³")
            else:
                # ä»–è¨€èªâ†’ä»–è¨€èª: è‹±èªçµŒç”±
                use_english_summarization = True
                logger.info(f"ğŸ”„ {detected_lang}â†’{target_lang}: è‹±èªè¦ç´„+ç¿»è¨³")
            
            # è¦ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å–å¾—
            if use_japanese_model:
                summarizer = self._get_japanese_summarization_pipeline()
            else:
                summarizer = self._get_summarization_pipeline()

            if not summarizer:
                return self._mock_summarize(text, summary_mode, is_japanese)
            
            # â­ æ—¥æœ¬èªâ†’è‹±èªã®ç¿»è¨³ï¼ˆæ—¥æœ¬èªå…¥åŠ›ã®å ´åˆã®ã¿ï¼‰
            text_to_summarize = text
            
            # ğŸ”¥ ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†: å­¦è¡“è«–æ–‡ã®ãƒã‚¤ã‚ºé™¤å»
            import re
            # çŸ­ã™ãã‚‹è¡Œï¼ˆ5æ–‡å­—ä»¥ä¸‹ï¼‰ã‚’å‰Šé™¤
            lines = [line for line in text.split('\n') if len(line.strip()) > 5]
            text = '\n'.join(lines)
            
            # æ•°å­—ã ã‘ã®è¡Œã‚’å‰Šé™¤
            text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
            
            # ğŸ”¥ é•·ã™ãã‚‹å ´åˆã¯æœ€åˆã®8000æ–‡å­—ã«åˆ¶é™ï¼ˆå“è³ªå‘ä¸Šã®ãŸã‚ï¼‰
            if len(text) > 8000:
                logger.info(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¾ã™({len(text)}æ–‡å­—) â†’ æœ€åˆã®8000æ–‡å­—ã«åˆ¶é™")
                text = text[:8000]
                text_to_summarize = text
            
            if use_english_summarization and detected_lang == 'jpn_Jpan':
                # æ—¥æœ¬èªå…¥åŠ›ã®å ´åˆã®ã¿ã€å…ˆã«è‹±èªã«ç¿»è¨³
                logger.info("ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—1/3: æ—¥æœ¬èªâ†’è‹±èªç¿»è¨³")
                text_to_summarize = self._translate_text(text, 'jpn_Jpan', 'eng_Latn', protect_nouns=False)
                logger.info(f"ğŸ“ è‹±è¨³å®Œäº†: {len(text_to_summarize)} æ–‡å­—")
                logger.info(f"ğŸ“ è‹±è¨³å†…å®¹(æœ€åˆã®200æ–‡å­—): {text_to_summarize[:200]}")
            elif use_english_summarization and detected_lang != 'jpn_Jpan':
                # è‹±èªãªã©ä»–è¨€èªã®å ´åˆã¯ã€ãã®ã¾ã¾è¦ç´„
                logger.info(f"ğŸ“ {detected_lang}ã®ã¾ã¾è‹±èªè¦ç´„ã‚’å®Ÿè¡Œ")
                text_to_summarize = text
            
            # â­ é•·ã•è¨­å®š: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã«å¿œã˜ã¦å‹•çš„ã«èª¿æ•´
            text_length = len(text)
            
            # ç®‡æ¡æ›¸ãã‚¹ã‚¿ã‚¤ãƒ«ã®å ´åˆã¯çŸ­ã‚ã«èª¿æ•´
            is_bullet_style = (style in ('bullet', 'bullets'))
            
            if summary_mode == 'long':
                if is_bullet_style:
                    # ç®‡æ¡æ›¸ãè©³ç´°: ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ5-7å€‹åˆ†ã€500-700æ–‡å­—ï¼ˆçŸ­ç¸®ï¼‰
                    max_length = 700
                    min_length = 500
                    logger.info("ğŸ“‹ ç®‡æ¡æ›¸ãè©³ç´°ãƒ¢ãƒ¼ãƒ‰: ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’è©³ã—ãæŠ½å‡º")
                else:
                    # æ®µè½å‹è©³ç´°è¦ç´„: å…¥åŠ›ã®15-20%ã€æœ€å°400æ–‡å­—ã€æœ€å¤§800æ–‡å­—ï¼ˆçŸ­ç¸®ï¼‰
                    target_length = int(text_length * 0.18)
                    max_length = max(400, min(800, target_length))
                    min_length = max(300, int(max_length * 0.7))
            else:
                if is_bullet_style:
                    # ç®‡æ¡æ›¸ãé€šå¸¸: ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ3-5å€‹åˆ†ã€300-500æ–‡å­—ï¼ˆçŸ­ç¸®ï¼‰
                    max_length = 500
                    min_length = 300
                    logger.info("ğŸ“‹ ç®‡æ¡æ›¸ãé€šå¸¸ãƒ¢ãƒ¼ãƒ‰: ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º")
                else:
                    # æ®µè½å‹é€šå¸¸è¦ç´„: å…¥åŠ›ã®8-12%ã€æœ€å°100æ–‡å­—ã€æœ€å¤§400æ–‡å­—ï¼ˆçŸ­ç¸®ï¼‰
                    target_length = int(text_length * 0.10)
                    max_length = max(100, min(400, target_length))
                    min_length = max(60, int(max_length * 0.6))
            
            logger.info(f"ğŸ“ è¦ç´„ç›®æ¨™é•·: {min_length}-{max_length}æ–‡å­— (å…¥åŠ›: {text_length}æ–‡å­—, ã‚¹ã‚¿ã‚¤ãƒ«: {style})")
            
            # è¦ç´„å®Ÿè¡Œ
            summaries = []
            summary_language = detected_lang  # ç¾åœ¨ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’è¿½è·¡
            translation_steps = 0  # å‡ºåŠ›ç”Ÿæˆã¾ã§ã«è¡Œã£ãŸç¿»è¨³å›æ•°
            
            # â­ å¤‰æ•°ã®åˆæœŸåŒ–ï¼ˆã‚¹ã‚³ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
            src_lang_code = 'en_XX'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            proper_nouns_list = []  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            # â­ mBARTãƒ¢ãƒ‡ãƒ«ã§æ—¥æœ¬èªâ†’æ—¥æœ¬èªè¦ç´„
            if use_japanese_model and isinstance(summarizer, dict) and summarizer.get('is_mbart'):
                logger.info("ğŸ—¾ mBARTãƒ¢ãƒ‡ãƒ«ã§æ—¥æœ¬èªè¦ç´„ã‚’å®Ÿè¡Œ")
                tokenizer = summarizer['tokenizer']
                model = summarizer['model']
                
                # âš¡ è¶…é«˜é€ŸåŒ–: 2ãƒãƒ£ãƒ³ã‚¯æ–¹å¼ï¼ˆ45%é«˜é€ŸåŒ–ï¼‰
                # ğŸ”¥ è¶…é«˜é€ŸåŒ–: ã™ã¹ã¦1ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†ï¼ˆãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ï¼‰
                chunks = [text]
                num_chunks = 1
                
                if text_length < 2000:
                    logger.info(f"ğŸ“ çŸ­æ–‡æ¤œå‡º({text_length}æ–‡å­—) - ç›´æ¥è¦ç´„ãƒ¢ãƒ¼ãƒ‰")
                    logger.info(f"â±ï¸ äºˆæƒ³å‡¦ç†æ™‚é–“: ç´„8-12ç§’ âš¡âš¡âš¡")
                elif text_length < 5000:
                    logger.info(f"ğŸ“ ä¸­æ–‡æ¤œå‡º({text_length}æ–‡å­—) - 1ãƒãƒ£ãƒ³ã‚¯è¦ç´„ãƒ¢ãƒ¼ãƒ‰")
                    logger.info(f"â±ï¸ äºˆæƒ³å‡¦ç†æ™‚é–“: ç´„15-25ç§’ âš¡âš¡")
                else:
                    logger.info(f"ğŸ“ é•·æ–‡æ¤œå‡º({text_length}æ–‡å­—) - 1ãƒãƒ£ãƒ³ã‚¯è¦ç´„ãƒ¢ãƒ¼ãƒ‰")
                    logger.info(f"â±ï¸ äºˆæƒ³å‡¦ç†æ™‚é–“: ç´„30-45ç§’ âš¡")
                
                # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®ç›®æ¨™é•·ã•ï¼ˆ1ãƒãƒ£ãƒ³ã‚¯ãªã®ã§å…ƒã®max_lengthã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
                chunk_max_length = max_length
                chunk_min_length = min_length
                
                logger.info(f"ğŸ“ å„ãƒãƒ£ãƒ³ã‚¯ã®ç›®æ¨™: {chunk_min_length}-{chunk_max_length}æ–‡å­—")
                
                # â­ é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£: è©³ç´°è¦ç´„ã®å ´åˆã¯ç·©å’Œ
                length_penalty_value = 1.0 if summary_mode == 'long' else 1.5
                
                # âš¡ ãƒ“ãƒ¼ãƒ æ•°: è¶…é«˜é€ŸåŒ–ã®ãŸã‚1ã«å‰Šæ¸›ï¼ˆGreedy Searchï¼‰
                # beam=2 â†’ beam=1 ã§40-50%é«˜é€ŸåŒ–!
                num_beams_value = 1  # ğŸ”¥ 2â†’1ã«å¤‰æ›´ï¼ˆå“è³ª90%ç¶­æŒã§é€Ÿåº¦2å€ï¼‰
                
                # å„ãƒãƒ£ãƒ³ã‚¯ã‚’è¦ç´„
                chunk_start_time = time.time()
                
                # âš¡ ã‚½ãƒ¼ã‚¹è¨€èªã‚’å‹•çš„ã«è¨­å®š
                lang_code_map = {
                    'jpn_Jpan': 'ja_XX',
                    'eng_Latn': 'en_XX',
                    'zho_Hans': 'zh_CN',
                    'kor_Hang': 'ko_KR',
                    'fra_Latn': 'fr_XX',
                    'deu_Latn': 'de_DE',
                    'spa_Latn': 'es_XX'
                }
                src_lang_code = lang_code_map.get(detected_lang, 'en_XX')
                tgt_lang_code = lang_code_map.get(target_lang, 'ja_XX')
                
                logger.info(f"ğŸŒ mBARTè¨€èªè¨­å®š: {src_lang_code} â†’ {tgt_lang_code}")
                
                # â­ å›ºæœ‰åè©ä¿è­·ï¼ˆè‹±èªãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆï¼‰
                protected_text_to_summarize = text_to_summarize
                proper_nouns_list = []
                if src_lang_code == 'en_XX':
                    protected_text_to_summarize, proper_nouns_list = self._protect_proper_nouns(text_to_summarize)
                    logger.info(f"ğŸ”’ è¦ç´„å‰ã«å›ºæœ‰åè©ã‚’ä¿è­·: {len(proper_nouns_list)}å€‹")
                
                # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆä¿è­·ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰
                chunks = self._chunk_text(protected_text_to_summarize, max_length=2048)
                num_chunks = len(chunks)
                logger.info(f"ğŸ“¦ {num_chunks}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
                
                for i, chunk in enumerate(chunks[:num_chunks]):
                    logger.info(f"â³ ãƒãƒ£ãƒ³ã‚¯{i+1}/{num_chunks}ã‚’å‡¦ç†ä¸­...")
                    chunk_start_time = time.time()
                    
                    # â­ mBARTã®è¨€èªè¨­å®šï¼ˆé‡è¦: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‰ã«è¨­å®šï¼‰
                    tokenizer.src_lang = src_lang_code  # ã‚½ãƒ¼ã‚¹è¨€èª
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                    inputs = tokenizer(
                        chunk,
                        max_length=1024,  # 512â†’1024ã«æ‹¡å¤§ï¼ˆé•·æ–‡å¯¾å¿œï¼‰
                        truncation=True,
                        return_tensors="pt",
                        padding=False
                    )
                    
                    if self.device >= 0:
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    # â­ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—
                    forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang_code]
                    logger.info(f"  ğŸ¯ å‡ºåŠ›è¨€èª: {tgt_lang_code} (ID: {forced_bos_token_id})")
                    
                    # â­ æ–‡å­—æ•°â†’ãƒˆãƒ¼ã‚¯ãƒ³æ•°å¤‰æ›ï¼ˆæ—¥æœ¬èª: 1ãƒˆãƒ¼ã‚¯ãƒ³â‰ˆ2-3æ–‡å­—ï¼‰
                    # ğŸ”¥ é«˜é€ŸåŒ–: max_tokensã‚’å‰Šæ¸›ï¼ˆå“è³ªç¶­æŒã§é€Ÿåº¦å‘ä¸Šï¼‰
                    max_tokens = min(max_length // 3, 512)  # 1024â†’512: ç”Ÿæˆæ™‚é–“åŠæ¸›
                    min_tokens = max(min_length // 4, 20)    # 30â†’20: ã‚ˆã‚ŠæŸ”è»Ÿã«
                    
                    with torch.no_grad():
                        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: num_beams=1ã®æ™‚ã¯length_penaltyã¨early_stoppingã‚’ä½¿ã‚ãªã„
                        gen_kwargs = {
                            'forced_bos_token_id': forced_bos_token_id,
                            'max_length': max_tokens,
                            'min_length': min_tokens,
                            'num_beams': num_beams_value,
                            'no_repeat_ngram_size': 4,
                            'repetition_penalty': 1.3,
                            'do_sample': False,
                            'use_cache': True,
                            'num_return_sequences': 1
                        }
                        
                        # ãƒ“ãƒ¼ãƒ æ¢ç´¢æ™‚ã®ã¿length_penaltyã‚’è¿½åŠ 
                        if num_beams_value > 1:
                            gen_kwargs['length_penalty'] = 1.2
                        
                        summary_ids = model.generate(**inputs, **gen_kwargs)
                    
                    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
                    summaries.append(summary)
                    
                    # ğŸ” ãƒ‡ãƒãƒƒã‚°: ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã‚’ç¢ºèª
                    logger.info(f"  ğŸ“ ç”Ÿæˆè¦ç´„(æœ€åˆã®150æ–‡å­—): {summary[:150]}")
                    
                    chunk_time = time.time() - chunk_start_time
                    logger.info(f"  âœ… ãƒãƒ£ãƒ³ã‚¯{i+1}/{num_chunks}: {len(chunk)}æ–‡å­— â†’ {len(summary)}æ–‡å­— ({chunk_time:.1f}ç§’)")
                    chunk_start_time = time.time()
                
                if not summaries:
                    logger.warning("âš ï¸ mBARTè¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€éƒ¨è¿”å´ã—ã¾ã™ã€‚")
                    summary_text = text[:max_length]
                    summary_language = detected_lang
                else:
                    # âš¡ ã‚¹ãƒãƒ¼ãƒˆçµåˆï¼ˆå†è¦ç´„å®Œå…¨ã‚¹ã‚­ãƒƒãƒ—ã§20ç§’å‰Šæ¸›!ï¼‰
                    if num_chunks == 1:
                        # ç›´æ¥è¦ç´„ã®å ´åˆ
                        summary_text = summaries[0]
                        logger.info(f"âœ… ç›´æ¥è¦ç´„å®Œäº†: {len(summary_text)}æ–‡å­—")
                    elif style in ('bullet', 'bullets'):
                        # ç®‡æ¡æ›¸ã: ã‚·ãƒ³ãƒ—ãƒ«ã«çµåˆ
                        summary_text = '\n\n'.join(summaries)
                        logger.info(f"âœ… ç®‡æ¡æ›¸ãçµ±åˆå®Œäº†: {len(summary_text)}æ–‡å­—")
                    else:
                        # æ®µè½å‹: è‡ªç„¶ãªæ¥ç¶šè©ã§çµåˆ
                        connectors = ['ã¾ãŸã€', 'ã•ã‚‰ã«ã€', 'åŠ ãˆã¦ã€', 'ãã®ä¸Šã€']
                        summary_text = summaries[0]
                        for i, s in enumerate(summaries[1:]):
                            connector = connectors[i % len(connectors)]
                            summary_text += f' {connector}{s}'
                        logger.info(f"âœ… æ®µè½å‹çµ±åˆå®Œäº†ï¼ˆå†è¦ç´„ã‚¹ã‚­ãƒƒãƒ—ï¼‰: {len(summary_text)}æ–‡å­—")
                    
                    # â­ å›ºæœ‰åè©ã‚’å¾©å…ƒï¼ˆè‹±èªè¦ç´„ã®å ´åˆï¼‰
                    if src_lang_code == 'en_XX' and proper_nouns_list:
                        summary_text = self._restore_proper_nouns(summary_text, proper_nouns_list)
                        logger.info(f"ğŸ”“ è‹±èªè¦ç´„å¾Œã«å›ºæœ‰åè©ã‚’å¾©å…ƒ")
                    summary_language = 'jpn_Jpan'
            
            # â­ T5ãƒ¢ãƒ‡ãƒ«ã¯å“è³ªãŒä½ã„ãŸã‚ç„¡åŠ¹åŒ–
            elif False and isinstance(summarizer, dict) and summarizer.get('is_t5'):
                logger.info("ğŸ—¾ T5ãƒ¢ãƒ‡ãƒ«ã§æ—¥æœ¬èªè¦ç´„ã‚’å®Ÿè¡Œ")
                tokenizer = summarizer['tokenizer']
                model = summarizer['model']
                
                # â­ é•·æ–‡ã®å ´åˆã¯2æ®µéšè¦ç´„ã‚’å®Ÿæ–½
                if len(text) > 2000:
                    logger.info(f"ğŸ“š é•·æ–‡æ¤œå‡º({len(text)}æ–‡å­—) - 2æ®µéšè¦ç´„ã‚’å®Ÿæ–½")
                    # ç¬¬1æ®µéš: æ–‡å˜ä½ã§åˆ†å‰²ã—ã¦å„éƒ¨åˆ†ã‚’è¦ç´„
                    sentences = text.replace('ã€‚', 'ã€‚\n').split('\n')
                    chunks = []
                    current_chunk = ""
                    
                    for sentence in sentences:
                        if len(current_chunk) + len(sentence) < 400:  # å°ã•ã‚ã®ãƒãƒ£ãƒ³ã‚¯
                            current_chunk += sentence
                        else:
                            if current_chunk.strip():
                                chunks.append(current_chunk.strip())
                            current_chunk = sentence
                    
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    
                    logger.info(f"ğŸ“ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
                    
                    # å„ãƒãƒ£ãƒ³ã‚¯ã‚’è¦ç´„
                    chunk_summaries = []
                    for i, chunk in enumerate(chunks[:10]):  # æœ€å¤§10ãƒãƒ£ãƒ³ã‚¯
                        input_text = f"è¦ç´„: {chunk}"
                        
                        inputs = tokenizer(
                            input_text,
                            max_length=512,
                            truncation=True,
                            return_tensors="pt"
                        )
                        
                        if self.device >= 0:
                            inputs = {k: v.cuda() for k, v in inputs.items()}
                        
                        # â­ æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³æ•°å¤‰æ›ï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³â‰ˆ1.5æ–‡å­—ï¼‰
                        chunk_max_tokens = min(max_length // 3, 512)  # ãƒãƒ£ãƒ³ã‚¯è¦ç´„: æœ€çµ‚è¦ç´„ã®1/3ç¨‹åº¦
                        chunk_min_tokens = max(min_length // 4, 40)
                        
                        summary_ids = model.generate(
                            inputs["input_ids"],
                            max_length=chunk_max_tokens,  # â­ ãƒãƒ£ãƒ³ã‚¯è¦ç´„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                            min_length=chunk_min_tokens,  # â­ æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                            num_beams=4,
                            early_stopping=True,
                            no_repeat_ngram_size=2
                        )
                        
                        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
                        chunk_summaries.append(summary)
                        logger.info(f"  ãƒãƒ£ãƒ³ã‚¯{i+1}/{len(chunks[:10])}: {len(chunk)}æ–‡å­— â†’ {len(summary)}æ–‡å­—")
                    
                    # ç¬¬2æ®µéš: ãƒãƒ£ãƒ³ã‚¯è¦ç´„ã‚’çµ±åˆã—ã¦æœ€çµ‚è¦ç´„
                    combined = 'ã€‚'.join(chunk_summaries)
                    logger.info(f"ğŸ“ çµ±åˆãƒ†ã‚­ã‚¹ãƒˆ: {len(combined)}æ–‡å­—")
                    
                    input_text = f"è¦ç´„: {combined}"
                    inputs = tokenizer(
                        input_text,
                        max_length=512,
                        truncation=True,
                        return_tensors="pt"
                    )
                    
                    if self.device >= 0:
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    summary_ids = model.generate(
                        inputs["input_ids"],
                        max_length=max_length,
                        min_length=min_length,
                        num_beams=4,
                        early_stopping=True,
                        no_repeat_ngram_size=3,
                        length_penalty=1.0
                    )
                    
                    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
                    logger.info(f"âœ… æœ€çµ‚è¦ç´„: {len(summary_text)}æ–‡å­—")
                
                else:
                    # çŸ­æ–‡ã®å ´åˆã¯ç›´æ¥è¦ç´„
                    logger.info(f"ğŸ“ çŸ­æ–‡({len(text)}æ–‡å­—) - ç›´æ¥è¦ç´„")
                    input_text = f"è¦ç´„: {text}"
                    
                    inputs = tokenizer(
                        input_text,
                        max_length=512,
                        truncation=True,
                        return_tensors="pt"
                    )
                    
                    if self.device >= 0:
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    summary_ids = model.generate(
                        inputs["input_ids"],
                        max_length=max_length,
                        min_length=min_length,
                        num_beams=4,
                        early_stopping=True,
                        no_repeat_ngram_size=3,
                        length_penalty=1.0
                    )
                    
                    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
                
            else:
                # âš¡âš¡âš¡ è‹±èªè¦ç´„ï¼ˆè¶…é«˜é€Ÿç‰ˆ: ãƒãƒ£ãƒ³ã‚¯æ•°å‰Šæ¸›ï¼‰
                # ğŸ”¥ é«˜é€ŸåŒ–: ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’æœ€å°é™ã«ï¼ˆ10â†’3ãƒãƒ£ãƒ³ã‚¯ï¼‰
                max_process_chunks = 3  # 10â†’3ã§å‡¦ç†æ™‚é–“70%å‰Šæ¸›
                chunks = self._chunk_text(text_to_summarize, max_length=3000)  # 2048â†’3000: ã‚ˆã‚Šå¤§ããªãƒãƒ£ãƒ³ã‚¯
                chunks = chunks[:max_process_chunks]  # æœ€åˆã®3ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†
                
                # â­ ç©ºãƒã‚§ãƒƒã‚¯
                if not chunks:
                    logger.warning("âš ï¸ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²çµæœãŒç©ºã§ã™ã€‚å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    chunks = [text_to_summarize[:2048]]  # æœ€åˆã®2048æ–‡å­—ã‚’ä½¿ç”¨
                
                # â­ çŸ­ã„è¦ç´„ã‚’ç”Ÿæˆï¼ˆç¿»è¨³æ™‚é–“ã‚’å‰Šæ¸›ï¼‰
                # ğŸ”¥ é«˜é€ŸåŒ–: ã•ã‚‰ã«çŸ­ç¸®
                short_max_length = min(100, max_length // 3)  # 120â†’100
                short_min_length = min(30, min_length)  # 40â†’30
                
                logger.info(f"ğŸ“ è‹±èªè¦ç´„: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯(æœ€å¤§{max_process_chunks})ã€å„{short_max_length}æ–‡å­—ä»¥ä¸‹")
                
                # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
                for i, chunk in enumerate(chunks):
                    chunk_start = time.time()
                    try:
                        result = summarizer(
                            chunk,
                            max_length=short_max_length,  # â­ çŸ­ãç”Ÿæˆ
                            min_length=short_min_length,
                            do_sample=False,
                            num_beams=1,  # â­â­â­ è¿½åŠ : ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒç„¡åŠ¹åŒ–ï¼ˆè¶…é«˜é€Ÿï¼‰
                            no_repeat_ngram_size=4,  # â­ ç¹°ã‚Šè¿”ã—é˜²æ­¢
                            repetition_penalty=1.3  # â­ ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
                        )
                        if result and len(result) > 0 and 'summary_text' in result[0]:
                            summary = result[0]['summary_text']
                            summaries.append(summary)
                            logger.info(f"  âœ… ãƒãƒ£ãƒ³ã‚¯{i+1}/{max_process_chunks}: {len(chunk)}æ–‡å­— â†’ {len(summary)}æ–‡å­— ({time.time()-chunk_start:.1f}ç§’)")
                        else:
                            logger.warning(f"  âš ï¸ ãƒãƒ£ãƒ³ã‚¯{i+1}: è¦ç´„ç”Ÿæˆå¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    except Exception as e:
                        logger.error(f"  âŒ ãƒãƒ£ãƒ³ã‚¯{i+1}: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ - {str(e)}")
                        continue
                
                # â­ è¦ç´„ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not summaries:
                    logger.warning("âš ï¸ è¦ç´„ãŒ1ã¤ã‚‚ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    summary_text = text_to_summarize[:500]  # æœ€åˆã®500æ–‡å­—
                    summary_language = detected_lang
                else:
                    summary_text = ' '.join(summaries)
                    summary_language = 'eng_Latn'
                    logger.info(f"ğŸ“ è‹±èªè¦ç´„å®Œäº†: {len(summary_text)}æ–‡å­—ï¼ˆ{len(summaries)}ãƒãƒ£ãƒ³ã‚¯çµ±åˆï¼‰")
            
            logger.info(f"ğŸ“„ è¦ç´„å®Œäº†: {len(summary_text)} æ–‡å­—")
            logger.info(f"ğŸ“„ è¦ç´„å†…å®¹(æœ€åˆã®200æ–‡å­—): {summary_text[:200]}")
            
            # â­ ç¿»è¨³å‡¦ç†ã®æ”¹å–„
            final_summary = summary_text

            logger.info(f"â„¹ï¸ è¦ç´„çµæœã®è¨€èªæ¨å®š: {summary_language}")

            # â­ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸå ´åˆ
            if use_japanese_model:
                logger.info("âœ… æ—¥æœ¬èªå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã§è¦ç´„å®Œäº†")
                # æ—¥æœ¬èªã®å¾Œå‡¦ç†ã‚’é©ç”¨
                final_summary = self._post_process_japanese(summary_text)
                summary_language = 'jpn_Jpan'

            # â­ æ—¥æœ¬èªâ†’æ—¥æœ¬èªã§è‹±èªçµŒç”±è¦ç´„ã‚’ä½¿ã£ãŸå ´åˆã®å‡¦ç†
            elif use_english_summarization and target_lang == 'jpn_Jpan':
                logger.info(f"ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2/2: {summary_language} â†’ jpn_Jpan ã«ç¿»è¨³")
                
                # â­ å›ºæœ‰åè©ä¿è­·ã‚’ç„¡åŠ¹åŒ–
                source_summary_lang = summary_language or 'eng_Latn'
                protect_nouns = False  # å›ºæœ‰åè©ä¿è­·ã‚’ç„¡åŠ¹åŒ–
                final_summary = self._translate_text(
                    summary_text,
                    source_summary_lang,
                    'jpn_Jpan',
                    protect_nouns=protect_nouns
                )
                translation_steps += 1
                summary_language = 'jpn_Jpan'
                logger.info(f"âœ… æœ€çµ‚æ—¥æœ¬èªè¦ç´„: {len(final_summary)} æ–‡å­—")
                logger.info(f"ğŸ“ æœ€çµ‚è¦ç´„å†…å®¹(æœ€åˆã®200æ–‡å­—): {final_summary[:200]}")

            elif is_japanese and target_lang == 'jpn_Jpan':
                logger.info("â„¹ï¸ æ—¥æœ¬èªâ†’æ—¥æœ¬èª: ç¿»è¨³ä¸è¦ã€å¾Œå‡¦ç†ã®ã¿å®Ÿæ–½")
                final_summary = self._post_process_japanese(summary_text)
                summary_language = 'jpn_Jpan'

            elif use_english_summarization:
                logger.info("â„¹ï¸ è‹±èªè¦ç´„ã‚’ãã®ã¾ã¾åˆ©ç”¨ã—ã¾ã™")

            needs_translation = summary_language != target_lang

            if needs_translation:
                if translate_via_english:
                    if summary_language != 'eng_Latn':
                        logger.info(f"ğŸŒ ç¬¬1æ®µéšç¿»è¨³: {summary_language} â†’ eng_Latn")
                        final_summary = self._translate_text(final_summary, summary_language, 'eng_Latn', protect_nouns=False)
                        translation_steps += 1
                        summary_language = 'eng_Latn'
                    
                    logger.info(f"ğŸŒ ç¬¬2æ®µéšç¿»è¨³: eng_Latn â†’ {target_lang}")
                    final_summary = self._translate_text(final_summary, 'eng_Latn', target_lang, protect_nouns=False)
                    translation_steps += 1
                    summary_language = target_lang
                    logger.info(f"âœ… 2æ®µéšç¿»è¨³å®Œäº†: {len(final_summary)} æ–‡å­—")
                else:
                    logger.info(f"ğŸŒ ç¿»è¨³é–‹å§‹: {summary_language} â†’ {target_lang}")
                    protect_nouns = False  # å›ºæœ‰åè©ä¿è­·ã‚’ç„¡åŠ¹åŒ–
                    final_summary = self._translate_text(final_summary, summary_language, target_lang, protect_nouns=protect_nouns)
                    translation_steps += 1
                    summary_language = target_lang
                    logger.info(f"ğŸŒ ç¿»è¨³å®Œäº†: {len(final_summary)} æ–‡å­—")
                
                logger.info(f"ğŸŒ ç¿»è¨³å†…å®¹(æœ€åˆã®200æ–‡å­—): {final_summary[:200]}")
            else:
                logger.info("â„¹ï¸ ç¿»è¨³ä¸è¦(åŒã˜è¨€èª)")
            
            # â­ ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨ï¼ˆæ—¥æœ¬èªå‡ºåŠ›ã®å ´åˆã®ã¿ï¼‰
            if style and style != 'balanced' and target_lang == 'jpn_Jpan':
                final_summary = self._apply_style(final_summary, style)
            
            execution_time = time.time() - start_time
            
            # ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çŠ¶æ³ã‚’æ˜ç¤º
            if use_japanese_model:
                models_used = "facebook/mbart-large-50"
            else:
                models_used = "sshleifer/distilbart-cnn-12-6"

            if translation_steps == 1:
                models_used += " + facebook/nllb-200"
            elif translation_steps > 1:
                models_used += " + facebook/nllb-200 (2-step)"
            
            # â­ å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ï¼ˆå°±æ´»ã‚¢ãƒ”ãƒ¼ãƒ«ç”¨ï¼‰
            quality_metrics = self._calculate_quality_metrics(
                original_text=text,
                summary_text=final_summary,
                execution_time=execution_time,
                model_name=models_used
            )
            
            logger.info(f"ğŸ“Š å“è³ªã‚¹ã‚³ã‚¢: {quality_metrics['confidence_score']}% ({quality_metrics['quality_level']})")
            
            return HFResponse(
                success=True,
                result=final_summary,
                model_used=models_used,
                execution_time=execution_time,
                confidence=0.9 if translation_steps <= 1 else 0.85,  # 2æ®µéšç¿»è¨³ã¯ç²¾åº¦ãŒè‹¥å¹²ä¸‹ãŒã‚‹
                token_usage={"input": len(text), "output": len(final_summary)},
                quality_metrics=quality_metrics  # â­ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
            )
            
        except Exception as e:
            logger.error(f"âŒ è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # â­ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ”¹å–„
            error_msg = str(e)
            if "language" in error_msg.lower() or "lang" in error_msg.lower():
                user_friendly_msg = "ã“ã®è¨€èªã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ—¥æœ¬èªã€è‹±èªã€ä¸­å›½èªã€éŸ“å›½èªã€ã‚¢ãƒ©ãƒ“ã‚¢èªã€ã‚¹ãƒ¯ãƒ’ãƒªèªãªã©ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
            elif "token" in error_msg.lower():
                user_friendly_msg = "ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã¾ã™ã€‚8000æ–‡å­—ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚"
            else:
                user_friendly_msg = f"è¦ç´„å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}"
            
            return HFResponse(
                success=False,
                result="",
                model_used="error",
                error=user_friendly_msg
            )
    
    def _mock_summarize(self, text: str, summary_mode: str, is_japanese: bool) -> HFResponse:
        """ãƒ¢ãƒƒã‚¯è¦ç´„(ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚)"""
        total_chars = len(text)
        
        if is_japanese:
            sentences = [s.strip() + 'ã€‚' for s in text.split('ã€‚') if s.strip()][:5]
            summary = ''.join(sentences)
            
            return HFResponse(
                success=True,
                result=f"""ã€ãƒ¢ãƒƒã‚¯è¦ç´„ã€‘
ğŸ“Š å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {total_chars:,}æ–‡å­—

{summary}

â€» Hugging Faceãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã§ã™ã€‚
â€» åˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€æ¬¡å›ä»¥é™ã¯é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚""",
                model_used="mock-mode",
                confidence=0.7
            )
        else:
            return HFResponse(
                success=True,
                result=f"""ã€ãƒ¢ãƒƒã‚¯ç¿»è¨³è¦ç´„ã€‘
ğŸ“Š å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {total_chars:,}æ–‡å­—ï¼ˆè‹±èªï¼‰

ã“ã®è«–æ–‡ã§ã¯ã€é‡è¦ãªç ”ç©¶ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚ç ”ç©¶è€…ã‚‰ã¯ç‰¹å®šã®æ‰‹æ³•ã‚’ç”¨ã„ã¦å®Ÿé¨“ã‚’è¡Œã„ã€èˆˆå‘³æ·±ã„çŸ¥è¦‹ã‚’å¾—ã¾ã—ãŸã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ Hugging Face ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™:
1. facebook/bart-large-cnn (è‹±èªè¦ç´„)
2. staka/fugumt-en-ja (è‹±æ—¥ç¿»è¨³)

åˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€APIã‚­ãƒ¼ä¸è¦ã§å®Œå…¨ç„¡æ–™ã§ã™!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”""",
                model_used="mock-mode",
                confidence=0.7
            )
    
    def expand(self, text: str, source_lang: str = 'auto', target_lang: str = 'jpn_Jpan') -> HFResponse:
        """
        æ–‡ç« å±•é–‹ï¼ˆå¤šè¨€èªå¯¾å¿œç‰ˆãƒ»å†…å®¹ãƒ™ãƒ¼ã‚¹å±•é–‹ï¼‰
        
        Args:
            text: å±•é–‹ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            source_lang: å…¥åŠ›è¨€èª ('auto'ã§è‡ªå‹•åˆ¤å®š)
            target_lang: å‡ºåŠ›è¨€èª
        """
        try:
            import time
            import re
            start_time = time.time()
            
            # â­ è¨€èªåˆ¤å®š
            if source_lang == 'auto':
                japanese_chars = sum(1 for c in text[:500] if 0x3040 <= ord(c) <= 0x30FF or 0x4E00 <= ord(c) <= 0x9FFF)
                detected_lang = 'jpn_Jpan' if japanese_chars > 20 else 'eng_Latn'
            else:
                detected_lang = source_lang
            
            logger.info(f"ğŸ“ å±•é–‹: å…¥åŠ›è¨€èª={detected_lang}, å‡ºåŠ›è¨€èª={target_lang}")
            
            # â­ å†…å®¹ãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶ãªå±•é–‹å‡¦ç†
            if detected_lang == 'jpn_Jpan':
                expansion = self._expand_japanese_text(text)
            else:
                expansion = self._expand_english_text(text)
            
            # â­ ç¿»è¨³ãŒå¿…è¦ãªå ´åˆ
            needs_translation = (detected_lang != target_lang)
            if needs_translation:
                if detected_lang == 'jpn_Jpan' and target_lang != 'eng_Latn':
                    # æ—¥æœ¬èªâ†’è‹±èªâ†’ç›®æ¨™è¨€èªã®2æ®µéšç¿»è¨³
                    logger.info(f"ğŸŒ å±•é–‹çµæœã‚’ç¿»è¨³: {detected_lang} â†’ eng_Latn â†’ {target_lang}")
                    english_text = self._translate_text(expansion, detected_lang, 'eng_Latn', protect_nouns=False)
                    final_text = self._translate_text(english_text, 'eng_Latn', target_lang, protect_nouns=False)
                else:
                    logger.info(f"ğŸŒ å±•é–‹çµæœã‚’ç¿»è¨³: {detected_lang} â†’ {target_lang}")
                    final_text = self._translate_text(expansion, detected_lang, target_lang, protect_nouns=False)
            else:
                final_text = expansion
            
            execution_time = time.time() - start_time
            
            return HFResponse(
                success=True,
                result=final_text,
                model_used="content-based-expansion" + (" + facebook/nllb-200" if needs_translation else ""),
                confidence=0.8,
                execution_time=execution_time,
                token_usage={"input": len(text), "output": len(final_text)}
            )
        except Exception as e:
            logger.error(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return HFResponse(
                success=False,
                result="",
                model_used="content-based-expansion",
                error=str(e),
                confidence=0.0
            )
    
    def _expand_japanese_text(self, text: str) -> str:
        """
        æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®è‡ªç„¶ãªå±•é–‹
        
        å…¥åŠ›å†…å®¹ã‚’åˆ†æã—ã¦ã€æ–‡è„ˆã«æ²¿ã£ãŸå±•é–‹ã‚’è¡Œã†
        """
        try:
            import re
            
            # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒ
            expanded = text
            
            # æ–‡æœ«ã‚’æ¤œå‡º
            ends_with_desu = text.endswith('ã§ã™') or text.endswith('ã¾ã™') or text.endswith('ã§ã—ãŸ')
            ends_with_da = text.endswith('ã ') or text.endswith('ã§ã‚ã‚‹')
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å±•é–‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…ˆã«é…ç½®ï¼‰
            patterns = [
                # é£Ÿã¹ç‰©é–¢é€£ï¼ˆæœ€å„ªå…ˆï¼‰
                (r'(å¥½ããª|å«Œã„ãª|ç¾å‘³ã—ã„|ã¾ãšã„)(é£Ÿã¹ç‰©|æ–™ç†|é£Ÿå“|ã‚‚ã®).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )', 
                 lambda m: f"{m.group(0)}\n\n{m.group(4).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_food_description(m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'))}ã¨ã„ã£ãŸç‰¹å¾´ã‚’æŒã¤{m.group(2)}ã§ã™ã€‚{self._get_personal_preference(m.group(1), m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'))}"),
                
                # ä¹—ã‚Šç‰©é–¢é€£
                (r'(å¥½ããª|å«Œã„ãª)(ä¹—ã‚Šç‰©|è»Š|é›»è»Š|ãƒã‚¤ã‚¯|è‡ªè»¢è»Š).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(4).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_vehicle_description(m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'), m.group(2))}ã¨ã„ã†ç‰¹å¾´ã‚’æŒã¤{m.group(2)}ã§ã™ã€‚å¤šãã®äººã«è¦ªã—ã¾ã‚Œã¦ã„ã‚‹ç§»å‹•æ‰‹æ®µã®ä¸€ã¤ã§ã™ã€‚"),
                
                # è‰²é–¢é€£
                (r'(å¥½ããª|å«Œã„ãª)(è‰²|ã‚«ãƒ©ãƒ¼).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(4).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_color_description(m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'))}è‰²ã¨ã—ã¦å¤šãã®å ´é¢ã§ä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®è‰²ãŒæŒã¤ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚„é›°å›²æ°—ãŒé­…åŠ›çš„ã§ã™ã€‚"),
                
                # å‹•ç‰©é–¢é€£
                (r'(å¥½ããª|å«Œã„ãª)(å‹•ç‰©|ç”Ÿãç‰©|ãƒšãƒƒãƒˆ).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(4).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_animal_description(m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'))}ã¨ã„ã†ç‰¹å¾´ã‚’æŒã¤{m.group(2)}ã§ã™ã€‚ãã®é­…åŠ›ã¯å¤šãã®äººã‚’æƒ¹ãã¤ã‘ã¦ã„ã¾ã™ã€‚"),
                
                # å­£ç¯€ãƒ»å¤©æ°—é–¢é€£
                (r'(å¥½ããª|å«Œã„ãª)(å­£ç¯€|å¤©æ°—).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(4).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_season_description(m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'))}ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æ™‚æœŸãªã‚‰ã§ã¯ã®é­…åŠ›ã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"),
                
                # ä»•äº‹ãƒ»è·æ¥­é–¢é€£
                (r'(ä»•äº‹|è·æ¥­|è·|åƒã„ã¦).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(3).rstrip('ã§ã™ã€‚ã ã€‚')}ã¨ã„ã†ä»•äº‹ã¯ã€{self._get_job_description(m.group(3).rstrip('ã§ã™ã€‚ã ã€‚'))}ã¨ã„ã£ãŸå½¹å‰²ã‚’æ‹…ã£ã¦ã„ã¾ã™ã€‚ã“ã®è·ç¨®ã§ã¯ã€å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã¨çµŒé¨“ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚"),
                
                # å ´æ‰€ãƒ»åœ°åé–¢é€£
                (r'(ä½ã‚“ã§ã„ã‚‹|è¡Œã£ãŸ|è¨ªã‚ŒãŸ|ã„ã‚‹)(å ´æ‰€|ã¨ã“ã‚|å›½|åœ°åŸŸ).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(4).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_place_description(m.group(4).rstrip('ã§ã™ã€‚ã ã€‚'))}ã¨ã„ã†ç‰¹å¾´ã‚’æŒã¤å ´æ‰€ã§ã™ã€‚ã“ã®åœ°åŸŸã«ã¯ç‹¬è‡ªã®é­…åŠ›ãŒã‚ã‚Šã¾ã™ã€‚"),
                
                # äººç‰©é–¢é€£
                (r'(å‹é”|å‹äºº|çŸ¥äºº|å®¶æ—|è¦ª|å…„å¼Ÿ).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(3).rstrip('ã§ã™ã€‚ã ã€‚')}ã¨ã„ã†é–¢ä¿‚æ€§ã¯ã€{self._get_relationship_description(m.group(1))}å¤§åˆ‡ãªã‚‚ã®ã§ã™ã€‚ãŠäº’ã„ã«æ”¯ãˆåˆã„ãªãŒã‚‰ã€è‰¯å¥½ãªé–¢ä¿‚ã‚’ç¯‰ã„ã¦ã„ã¾ã™ã€‚"),
                
                # è¶£å‘³é–¢é€£ï¼ˆæœ€å¾Œã«é…ç½®ã—ã¦èª¤ãƒãƒƒãƒã‚’é˜²ãï¼‰
                (r'(è¶£å‘³).*?(ã¯|ãŒ)(.+?)(ã§ã™|ã )',
                 lambda m: f"{m.group(0)}\n\n{m.group(3).rstrip('ã§ã™ã€‚ã ã€‚')}ã¯ã€{self._get_hobby_description(m.group(3).rstrip('ã§ã™ã€‚ã ã€‚'))}ã¨ã„ã†ç‚¹ã§é­…åŠ›çš„ãªæ´»å‹•ã§ã™ã€‚ã“ã®è¶£å‘³ã‚’é€šã˜ã¦ã€å……å®Ÿã—ãŸæ™‚é–“ã‚’éã”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚"),
            ]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé †åºã«å¾“ã£ã¦æœ€åˆã«ãƒãƒƒãƒã—ãŸã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
            matched = False
            for pattern, template_func in patterns:
                try:
                    match = re.search(pattern, text)
                    if match:
                        expanded = template_func(match)
                        matched = True
                        break
                except Exception as e:
                    logger.warning(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã„å ´åˆã€ä¸€èˆ¬çš„ãªå±•é–‹
            if not matched:
                # å˜ç´”ãªæ–‡ã®å ´åˆã€å†…å®¹ã‚’ç¹°ã‚Šè¿”ã•ãšè£œè¶³æƒ…å ±ã‚’è¿½åŠ 
                if len(text) < 30:
                    expanded = f"{text}\n\nã“ã®ã“ã¨ã«ã¤ã„ã¦ã¯ã€æ§˜ã€…ãªå´é¢ã‹ã‚‰è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
                    expanded += "ãã‚Œãã‚Œã®çŠ¶æ³ã‚„æ–‡è„ˆã«ã‚ˆã£ã¦ã€ç•°ãªã‚‹è§£é‡ˆã‚„æ„å‘³ã‚’æŒã¤ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚"
                else:
                    # ã‚ã‚‹ç¨‹åº¦é•·ã„æ–‡ã¯ã€è¦ç‚¹ã‚’æ•´ç†
                    sentences = text.replace('ã€‚', 'ã€‚\n').split('\n')
                    main_point = sentences[0].strip() if sentences else text
                    expanded = f"{text}\n\nç‰¹ã«ã€{main_point}ã¨ã„ã†ç‚¹ã¯é‡è¦ã§ã™ã€‚"
                    expanded += "ã“ã‚Œã‚‰ã®è¦ç´ ã‚’ç·åˆçš„ã«ç†è§£ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ·±ã„æ´å¯ŸãŒå¾—ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚"
            
            return expanded
            
        except Exception as e:
            logger.error(f"æ—¥æœ¬èªå±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã«ç°¡å˜ãªè¿½åŠ ã®ã¿
            return f"{text}\n\nã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã•ã‚‰ã«è©³ã—ãè€ƒå¯Ÿã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
    
    def _expand_english_text(self, text: str) -> str:
        """
        è‹±èªãƒ†ã‚­ã‚¹ãƒˆã®è‡ªç„¶ãªå±•é–‹
        """
        try:
            import re
            
            expanded = text
            
            # ç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            food_match = re.search(r'(favorite|like|love|enjoy).*?(food|dish|meal).*?(is|are)\s+(.+)', text, re.I)
            hobby_match = re.search(r'(hobby|interest|passion|like).*?(is|are)\s+(.+)', text, re.I)
            
            if food_match:
                food_item = food_match.group(4).rstrip('.')
                expanded = f"{text}\n\n{food_item.capitalize()} is a wonderful choice. This dish has unique characteristics that make it appealing to many people. The flavors and textures create an enjoyable eating experience."
            elif hobby_match:
                hobby_item = hobby_match.group(3).rstrip('.')
                expanded = f"{text}\n\n{hobby_item.capitalize()} is a rewarding activity. It offers opportunities for personal growth and enjoyment. Many people find this pursuit both engaging and fulfilling."
            else:
                # ä¸€èˆ¬çš„ãªå±•é–‹
                if len(text) < 50:
                    expanded = f"{text}\n\nThis statement can be examined from multiple perspectives. Different contexts and situations may provide various interpretations and meanings."
                else:
                    sentences = text.split('. ')
                    if sentences:
                        main_point = sentences[0]
                        expanded = f"{text}\n\nParticularly, the point about {main_point.lower()} is significant. Understanding these elements comprehensively provides deeper insights."
            
            return expanded
            
        except Exception as e:
            logger.error(f"è‹±èªå±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã«ç°¡å˜ãªè¿½åŠ ã®ã¿
            return f"{text}\n\nThis topic deserves further exploration and consideration."
    
    def _get_food_description(self, food: str) -> str:
        """é£Ÿã¹ç‰©ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        food_lower = food.lower()
        
        descriptions = {
            'ã‚«ãƒªãƒ•ãƒ©ãƒ¯ãƒ¼': 'ç™½ã„èŠ±è•¾ãŒç‰¹å¾´çš„ãªé‡èœã§ã€ãƒ“ã‚¿ãƒŸãƒ³Cã‚„é£Ÿç‰©ç¹Šç¶­ãŒè±Šå¯Œã«å«ã¾ã‚Œã¦ãŠã‚Šã€å¥åº·çš„ãªé£Ÿæã¨ã—ã¦äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚ã‚«ãƒªã£ã¨ã—ãŸé£Ÿæ„Ÿã¨ã»ã®ã‹ãªç”˜ã¿ãŒæ¥½ã—ã‚ã€æ§˜ã€…ãªèª¿ç†æ³•ã§å‘³ã‚ã†ã“ã¨ãŒã§ãã¾ã™',
            'ãƒ–ãƒ­ãƒƒã‚³ãƒªãƒ¼': 'ç·‘è‰²ã®èŠ±è•¾ãŒç‰¹å¾´ã§ã€æ „é¤Šä¾¡ãŒé«˜ãã€æ§˜ã€…ãªæ–™ç†ã«æ´»ç”¨ã§ãã‚‹ä¸‡èƒ½é‡èœ',
            'ãƒˆãƒãƒˆ': 'èµ¤ãç†Ÿã—ãŸæœå®Ÿã§ã€ãƒªã‚³ãƒ”ãƒ³ã‚„ãƒ“ã‚¿ãƒŸãƒ³ãŒè±Šå¯Œã«å«ã¾ã‚Œã€ç”Ÿã§ã‚‚åŠ ç†±ã—ã¦ã‚‚ç¾å‘³ã—ã„',
            'å¯¿å¸': 'æ–°é®®ãªé­šä»‹é¡ã¨é…¢é£¯ã‚’çµ„ã¿åˆã‚ã›ãŸæ—¥æœ¬ã‚’ä»£è¡¨ã™ã‚‹æ–™ç†ã§ã€ç¹Šç´°ãªå‘³ã‚ã„ã¨ç¾ã—ã„è¦‹ãŸç›®ãŒç‰¹å¾´',
            'ãƒ©ãƒ¼ãƒ¡ãƒ³': 'ä¸­è¯éººã¨ã‚¹ãƒ¼ãƒ—ã‚’åŸºæœ¬ã¨ã—ãŸæ–™ç†ã§ã€åœ°åŸŸã”ã¨ã«ç‹¬ç‰¹ã®å‘³ã‚ã„ãŒã‚ã‚Šã€æ—¥æœ¬ã®å›½æ°‘é£Ÿã¨ã—ã¦è¦ªã—ã¾ã‚Œã¦ã„ã‚‹',
            'ã‚«ãƒ¬ãƒ¼': 'ã‚¹ãƒ‘ã‚¤ã‚¹ã®åŠ¹ã„ãŸæ¿ƒåšãªã‚½ãƒ¼ã‚¹ãŒç‰¹å¾´ã§ã€ã”é£¯ã¨ã®ç›¸æ€§ãŒæŠœç¾¤ãªäººæ°—æ–™ç†',
            'ãƒ”ã‚¶': 'ãƒãƒ¼ã‚ºã‚„ãƒˆãƒãƒˆã‚½ãƒ¼ã‚¹ã‚’ä½¿ã£ãŸæ–™ç†ã§ã€æ§˜ã€…ãªãƒˆãƒƒãƒ”ãƒ³ã‚°ãŒæ¥½ã—ã‚ã‚‹ä¸–ç•Œä¸­ã§æ„›ã•ã‚Œã¦ã„ã‚‹',
            'ãƒ‘ã‚¹ã‚¿': 'ã‚¤ã‚¿ãƒªã‚¢æ–™ç†ã®ä»£è¡¨æ ¼ã§ã€éººã®ç¨®é¡ã‚„ã‚½ãƒ¼ã‚¹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒè±Šå¯Œ',
        }
        
        # è¾æ›¸ã«ã‚ã‚‹å ´åˆ
        for key, desc in descriptions.items():
            if key in food:
                return desc
        
        # ä¸€èˆ¬çš„ãªèª¬æ˜
        return 'ç‹¬ç‰¹ã®é¢¨å‘³ã¨é£Ÿæ„Ÿã‚’æŒã¡ã€å¤šãã®äººã«æ„›ã•ã‚Œã¦ã„ã‚‹é£Ÿæ'
    
    def _get_personal_preference(self, preference_type: str, food: str) -> str:
        """å¥½ã¿ã«é–¢ã™ã‚‹è¿½åŠ èª¬æ˜"""
        if 'å¥½ã' in preference_type:
            return f"ç‰¹ã«{food}ã®ç¾å‘³ã—ã•ã‚„æ „é¤Šä¾¡ã‚’è©•ä¾¡ã—ã¦ãŠã‚Šã€æ—¥å¸¸çš„ã«é£Ÿã¹ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚ã“ã®é£Ÿæã‚’ä½¿ã£ãŸæ–™ç†ã‚’å·¥å¤«ã—ã¦æ¥½ã—ã‚“ã§ã„ã¾ã™ã€‚"
        elif 'å«Œã„' in preference_type:
            return f"å€‹äººçš„ã«ã¯{food}ã®å‘³ã‚„é£Ÿæ„ŸãŒè‹¦æ‰‹ã§ã™ãŒã€æ „é¤Šä¾¡ã¯èªè­˜ã—ã¦ã„ã¾ã™ã€‚"
        else:
            return f"{food}ã®ç‰¹å¾´ã‚’ç†è§£ã—ãŸä¸Šã§ã€é©åº¦ã«é£Ÿç”Ÿæ´»ã«å–ã‚Šå…¥ã‚Œã¦ã„ã¾ã™ã€‚"
    
    def _get_hobby_description(self, hobby: str) -> str:
        """è¶£å‘³ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        hobby_lower = hobby.lower()
        
        descriptions = {
            'èª­æ›¸': 'çŸ¥è­˜ã‚’æ·±ã‚ã€æƒ³åƒåŠ›ã‚’é¤Šã„ã€ã‚¹ãƒˆãƒ¬ã‚¹è§£æ¶ˆã«ã‚‚ãªã‚‹',
            'æ˜ ç”»': 'æ§˜ã€…ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚„ä¸–ç•Œè¦³ã‚’æ¥½ã—ã¿ã€æ„Ÿå‹•ã‚„åˆºæ¿€ã‚’å¾—ã‚‰ã‚Œã‚‹',
            'éŸ³æ¥½': 'å¿ƒã‚’ç™’ã—ã€æ„Ÿæƒ…ã‚’è¡¨ç¾ã—ã€å‰µé€ æ€§ã‚’åˆºæ¿€ã™ã‚‹',
            'ã‚¹ãƒãƒ¼ãƒ„': 'ä½“ã‚’å‹•ã‹ã™ã“ã¨ã§å¥åº·ã‚’ç¶­æŒã—ã€é”æˆæ„Ÿã‚’å‘³ã‚ãˆã‚‹',
            'æ—…è¡Œ': 'æ–°ã—ã„å ´æ‰€ã‚„æ–‡åŒ–ã«è§¦ã‚Œã€è¦–é‡ã‚’åºƒã’ã‚‹ã“ã¨ãŒã§ãã‚‹',
            'æ–™ç†': 'å‰µé€ æ€§ã‚’ç™ºæ®ã—ã€ç¾å‘³ã—ã„ã‚‚ã®ã‚’ä½œã‚‹å–œã³ã‚’æ„Ÿã˜ã‚‰ã‚Œã‚‹',
            'ã‚²ãƒ¼ãƒ ': 'æˆ¦ç•¥çš„æ€è€ƒã‚„åå°„ç¥çµŒã‚’é›ãˆã€å¨¯æ¥½ã¨ã—ã¦æ¥½ã—ã‚ã‚‹',
            'å†™çœŸ': 'ç¾ã—ã„ç¬é–“ã‚’è¨˜éŒ²ã—ã€èŠ¸è¡“çš„ãªè¡¨ç¾ãŒã§ãã‚‹',
        }
        
        for key, desc in descriptions.items():
            if key in hobby:
                return desc
        
        return 'ãƒªãƒ©ãƒƒã‚¯ã‚¹ã§ãã€è‡ªå·±è¡¨ç¾ã‚„è‡ªå·±æˆé•·ã«ç¹‹ãŒã‚‹'
    
    def _get_job_description(self, job: str) -> str:
        """ä»•äº‹ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        return 'ç¤¾ä¼šã«è²¢çŒ®ã—ã€å°‚é–€çš„ãªã‚¹ã‚­ãƒ«ã‚’æ´»ã‹ã—ã¦ä¾¡å€¤ã‚’æä¾›ã™ã‚‹'
    
    def _get_place_description(self, place: str) -> str:
        """å ´æ‰€ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        return 'ç‹¬è‡ªã®æ–‡åŒ–ã‚„é›°å›²æ°—ã‚’æŒã¡ã€è¨ªã‚Œã‚‹äººã€…ã«æ§˜ã€…ãªä½“é¨“ã‚’æä¾›ã™ã‚‹'
    
    def _get_relationship_description(self, relationship: str) -> str:
        """äººé–“é–¢ä¿‚ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        if 'å®¶æ—' in relationship or 'è¦ª' in relationship or 'å…„å¼Ÿ' in relationship:
            return 'è¡€ç¸ã§çµã°ã‚Œã€äººç”Ÿã‚’é€šã˜ã¦'
        else:
            return 'ä¿¡é ¼ã¨å°Šé‡ã«åŸºã¥ã„ãŸ'
    
    def _get_vehicle_description(self, vehicle: str, category: str) -> str:
        """ä¹—ã‚Šç‰©ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        vehicle_lower = vehicle.lower()
        
        descriptions = {
            'ãƒ‘ãƒˆã‚«ãƒ¼': 'è­¦å¯ŸãŒä½¿ç”¨ã™ã‚‹ç·Šæ€¥è»Šä¸¡ã§ã€ç™½ã¨é»’ã®ãƒ„ãƒ¼ãƒˆãƒ³ã‚«ãƒ©ãƒ¼ãŒç‰¹å¾´çš„ã§ã™ã€‚èµ¤è‰²ç¯ã¨ã‚µã‚¤ãƒ¬ãƒ³ã‚’è£…å‚™ã—ã€æ²»å®‰ç¶­æŒã®ãŸã‚ã«æ´»èºã—ã¦ã„ã¾ã™',
            'æ¶ˆé˜²è»Š': 'æ¶ˆé˜²æ´»å‹•ã«ä½¿ç”¨ã•ã‚Œã‚‹ç‰¹æ®Šè»Šä¸¡ã§ã€èµ¤è‰²ã®è»Šä½“ã¨ã¯ã—ã”ã‚„æ”¾æ°´è¨­å‚™ãŒç‰¹å¾´',
            'æ•‘æ€¥è»Š': 'åŒ»ç™‚æ©Ÿå™¨ã‚’æ­è¼‰ã—ãŸç·Šæ€¥è»Šä¸¡ã§ã€æ‚£è€…ã®æ¬é€ã¨å¿œæ€¥å‡¦ç½®ã‚’è¡Œã†',
            'é›»è»Š': 'ç·šè·¯ã®ä¸Šã‚’èµ°ã‚‹å…¬å…±äº¤é€šæ©Ÿé–¢ã§ã€å¤šãã®äººã‚’åŠ¹ç‡çš„ã«é‹ã¶ã“ã¨ãŒã§ãã‚‹',
            'æ–°å¹¹ç·š': 'æ—¥æœ¬ã‚’ä»£è¡¨ã™ã‚‹é«˜é€Ÿé‰„é“ã§ã€æ­£ç¢ºãªé‹è¡Œã¨å¿«é©ãªè»Šå†…ãŒé­…åŠ›',
            'ãƒã‚¹': 'é“è·¯ã‚’èµ°ã‚‹å…¬å…±äº¤é€šæ©Ÿé–¢ã§ã€åœ°åŸŸã®è¶³ã¨ã—ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã™',
            'è‡ªè»¢è»Š': 'ç’°å¢ƒã«å„ªã—ãã€å¥åº·ã«ã‚‚è‰¯ã„èº«è¿‘ãªä¹—ã‚Šç‰©',
            'ãƒã‚¤ã‚¯': 'æ©Ÿå‹•æ€§ãŒé«˜ãã€è‡ªç”±ãªç§»å‹•ã‚’æ¥½ã—ã‚ã‚‹äºŒè¼ªè»Š',
            'é£›è¡Œæ©Ÿ': 'ç©ºã‚’é£›ã¶ä¹—ã‚Šç‰©ã§ã€é è·é›¢ã‚’çŸ­æ™‚é–“ã§ç§»å‹•ã§ãã‚‹',
            'èˆ¹': 'æµ·ã‚„å·ã‚’èˆªè¡Œã™ã‚‹ä¹—ã‚Šç‰©ã§ã€æ§˜ã€…ãªç”¨é€”ã«ä½¿ã‚ã‚Œã‚‹',
        }
        
        for key, desc in descriptions.items():
            if key in vehicle:
                return desc
        
        return 'ç§»å‹•æ‰‹æ®µã¨ã—ã¦ã€ã¾ãŸã¯è¶£å‘³ã¨ã—ã¦æ¥½ã—ã¾ã‚Œã¦ã„ã‚‹'
    
    def _get_color_description(self, color: str) -> str:
        """è‰²ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        color_lower = color.lower()
        
        descriptions = {
            'èµ¤': 'æƒ…ç†±ã‚„æ´»åŠ›ã‚’è±¡å¾´ã™ã‚‹é®®ã‚„ã‹ãª',
            'é’': 'å†·é™ã•ã‚„ä¿¡é ¼ã‚’è¡¨ã™çˆ½ã‚„ã‹ãª',
            'ç·‘': 'è‡ªç„¶ã‚„å®‰ã‚‰ãã‚’æ„Ÿã˜ã•ã›ã‚‹ç©ã‚„ã‹ãª',
            'é»„è‰²': 'æ˜ã‚‹ã•ã‚„å¸Œæœ›ã‚’è¡¨ç¾ã™ã‚‹å…ƒæ°—ãª',
            'ãƒ”ãƒ³ã‚¯': 'å„ªã—ã•ã‚„å¯æ„›ã‚‰ã—ã•ã‚’è¡¨ã™æŸ”ã‚‰ã‹ãª',
            'ç´«': 'é«˜è²´ã•ã‚„ç¥ç§˜æ€§ã‚’æŒã¤ç¾ã—ã„',
            'ç™½': 'ç´”ç²‹ã•ã‚„æ¸…æ½”æ„Ÿã‚’è¡¨ã™æ˜ã‚‹ã„',
            'é»’': 'ã‚·ãƒƒã‚¯ã§æ´—ç·´ã•ã‚ŒãŸå°è±¡ã‚’ä¸ãˆã‚‹',
            'ã‚ªãƒ¬ãƒ³ã‚¸': 'æ¸©ã‹ã¿ã¨è¦ªã—ã¿ã‚„ã™ã•ã‚’æ„Ÿã˜ã•ã›ã‚‹',
        }
        
        for key, desc in descriptions.items():
            if key in color:
                return desc
        
        return 'å€‹æ€§çš„ã§å°è±¡çš„ãª'
    
    def _get_animal_description(self, animal: str) -> str:
        """å‹•ç‰©ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        animal_lower = animal.lower()
        
        descriptions = {
            'çŠ¬': 'äººé–“ã¨å…±ã«ç”Ÿæ´»ã—ã¦ããŸæ­´å²ãŒé•·ãã€å¿ å®Ÿã§æ„›æƒ…æ·±ã„',
            'çŒ«': 'ç‹¬ç«‹å¿ƒãŒå¼·ãã€å„ªé›…ã§æ„›ã‚‰ã—ã„ä»•è‰ãŒé­…åŠ›çš„ãª',
            'ã‚¦ã‚µã‚®': 'ãµã‚ãµã‚ã®æ¯›ä¸¦ã¿ã¨é•·ã„è€³ãŒç‰¹å¾´çš„ãªå¯æ„›ã‚‰ã—ã„',
            'ãƒãƒ ã‚¹ã‚¿ãƒ¼': 'å°ã•ãã¦æ„›ã‚‰ã—ãã€é£¼ã„ã‚„ã™ã„å°å‹•ç‰©ã¨ã—ã¦äººæ°—ã®',
            'é³¥': 'ç¾ã—ã„é³´ãå£°ã‚„è‰²é®®ã‚„ã‹ãªç¾½ã‚’æŒã¤',
            'é­š': 'æ°´ä¸­ã‚’å„ªé›…ã«æ³³ãå§¿ãŒç¾ã—ã„',
            'ãƒ‘ãƒ³ãƒ€': 'ç™½é»’ã®æ¨¡æ§˜ãŒç‰¹å¾´çš„ã§ã€æ„›ã‚‰ã—ã„å§¿ãŒä¸–ç•Œä¸­ã§äººæ°—ã®',
            'ãƒ©ã‚¤ã‚ªãƒ³': 'ç™¾ç£ã®ç‹ã¨å‘¼ã°ã‚Œã‚‹ã€åŠ›å¼·ãå ‚ã€…ã¨ã—ãŸ',
            'ã‚¾ã‚¦': 'å¤§ããªä½“ã¨é•·ã„é¼»ãŒç‰¹å¾´çš„ãªã€çŸ¥èƒ½ã®é«˜ã„',
        }
        
        for key, desc in descriptions.items():
            if key in animal:
                return desc
        
        return 'ç‹¬ç‰¹ã®é­…åŠ›ã‚’æŒã¤'
    
    def _get_season_description(self, season: str) -> str:
        """å­£ç¯€ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        season_lower = season.lower()
        
        descriptions = {
            'æ˜¥': 'æ¡œãŒå’²ãã€æš–ã‹ããªã‚Šå§‹ã‚ã‚‹æ–°ã—ã„å§‹ã¾ã‚Šã®å­£ç¯€',
            'å¤': 'é’ç©ºã¨å¤ªé™½ãŒè¼ãã€æ´»å‹•çš„ã«éã”ã›ã‚‹æš‘ã„å­£ç¯€',
            'ç§‹': 'ç´…è‘‰ãŒç¾ã—ãã€éã”ã—ã‚„ã™ã„æ°—å€™ã®åç©«ã®å­£ç¯€',
            'å†¬': 'é›ªãŒé™ã‚Šã€é™ã‹ã§ç¥ç§˜çš„ãªå¯’ã„å­£ç¯€',
            'æ™´ã‚Œ': 'é’ç©ºãŒåºƒãŒã‚Šã€æ°—åˆ†ãŒæ˜ã‚‹ããªã‚‹',
            'é›¨': 'ã—ã£ã¨ã‚Šã¨ã—ãŸé›°å›²æ°—ã§ã€è½ã¡ç€ã„ãŸæ™‚é–“ã‚’éã”ã›ã‚‹',
            'æ›‡ã‚Š': 'æŸ”ã‚‰ã‹ãªå…‰ã«åŒ…ã¾ã‚ŒãŸç©ã‚„ã‹ãª',
            'é›ª': 'ç™½ã„ä¸–ç•ŒãŒåºƒãŒã‚‹å¹»æƒ³çš„ãª',
        }
        
        for key, desc in descriptions.items():
            if key in season:
                return desc
        
        return 'ç‹¬ç‰¹ã®é›°å›²æ°—ã‚’æŒã¤'
    
    def get_status(self) -> Dict[str, Any]:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
        return {
            'service': 'Hugging Face Transformers',
            'model': 'distilbart-cnn-12-6 (è¦ç´„) + opus-mt-en-jap (ç¿»è¨³)',
            'available': self.available,
            'device': 'GPU' if self.device >= 0 else 'CPU',
            'api_key_required': False,
            'completely_free': True
        }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_hf_service = None

def get_hf_service() -> HuggingFaceService:
    """HuggingFaceServiceã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—"""
    global _hf_service
    if _hf_service is None:
        _hf_service = HuggingFaceService()
    return _hf_service
