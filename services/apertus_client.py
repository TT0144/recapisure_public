#!/usr/bin/env python3
"""
Apertus API Client for recapisure
Apertus AIãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨ã®é€£æºã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""

import os
import json
import time
import logging
import requests
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """ä½¿ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"""
    GPT4_TURBO = "gpt-4-turbo"
    GPT4 = "gpt-4"
    GPT35_TURBO = "gpt-3.5-turbo"
    CLAUDE_3_SONNET = "claude-3-sonnet"
    CLAUDE_3_HAIKU = "claude-3-haiku"
    GEMINI_PRO = "gemini-pro"

class TaskType(Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—"""
    SUMMARIZE = "summarize"
    EXPAND = "expand"
    ANALYZE = "analyze"
    TRANSLATE = "translate"

@dataclass
class ApertusRequest:
    """Apertus APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    text: str
    task_type: TaskType
    model: ModelType
    max_length: Optional[int] = None
    min_length: Optional[int] = None
    temperature: float = 0.7
    language: str = "ja"
    custom_prompt: Optional[str] = None

@dataclass
class ApertusResponse:
    """Apertus APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    result: str
    model_used: str
    execution_time: float
    token_usage: Dict[str, int]
    confidence: float
    metadata: Dict[str, Any]
    error: Optional[str] = None

class ApertusClient:
    """Apertus LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (swiss-ai/Apertus-8B-Instruct-2509)"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        Apertus ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        
        Note:
            api_key, base_urlã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã—ã¦ã„ã¾ã™ãŒã€
            å®Ÿéš›ã«ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®Apertus LLMã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
        """
        # ãƒ­ãƒ¼ã‚«ãƒ«Apertusã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
        from .apertus_service import get_apertus_service
        self.apertus = get_apertus_service()
        self.mock_mode = not self.apertus.available
        
        if self.mock_mode:
            logger.warning("âš ï¸ Apertus LLMåˆ©ç”¨ä¸å¯ã€‚ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ã€‚")
        else:
            logger.info("âœ… Apertus LLM (swiss-ai/Apertus-8B-Instruct-2509) ã‚’ä½¿ç”¨")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.default_model = ModelType.GPT4_TURBO  # äº’æ›æ€§ã®ãŸã‚æ®‹ã™
        self.request_timeout = 30
        self.max_retries = 3
    
    # æ—§APIå‘¼ã³å‡ºã—ãƒ¡ã‚½ãƒƒãƒ‰å‰Šé™¤ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«LLMä½¿ç”¨ã®ãŸã‚ä¸è¦ï¼‰
    
    def _create_prompt(self, request: ApertusRequest) -> str:
        """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰"""
        if request.custom_prompt:
            return request.custom_prompt
        
        # è¨€èªåˆ¤å®š
        japanese_chars = sum(1 for c in request.text[:1000] if ord(c) > 0x3000)
        is_japanese = japanese_chars > 50
        
        if request.task_type == TaskType.SUMMARIZE:
            if is_japanese:
                # æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„
                return f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{request.min_length or 50}æ–‡å­—ä»¥ä¸Š{request.max_length or 200}æ–‡å­—ä»¥ä¸‹ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ¼ã‚‰ã•ãšã€ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{request.text}

è¦ç´„:
"""
            else:
                # è‹±èªãƒ†ã‚­ã‚¹ãƒˆ â†’ æ—¥æœ¬èªç¿»è¨³è¦ç´„
                max_len = request.max_length or 400
                return f"""
ä»¥ä¸‹ã®è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸä¸Šã§ã€ç´„{max_len}æ–‡å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

å‡¦ç†æ‰‹é †:
1. ã¾ãšè‹±èªã®å†…å®¹ã‚’æ­£ç¢ºã«ç†è§£ã™ã‚‹
2. ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹æ„Ÿè¦šã§èª­ã¿è§£ã
3. ç¿»è¨³ã•ã‚ŒãŸå†…å®¹ã‹ã‚‰é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦è¦ç´„ã™ã‚‹

è¦æ±‚äº‹é …:
- è«–æ–‡ã®ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§æ˜ç¢ºã«è¨˜è¿°
- å°‚é–€ç”¨èªã¯é©åˆ‡ãªæ—¥æœ¬èªè¨³ã‚’ä½¿ç”¨ï¼ˆå¿…è¦ã«å¿œã˜ã¦è‹±èªã‚’ä½µè¨˜ï¼‰
- è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã®è¦ç´„ã«ã™ã‚‹
- è«–æ–‡ã®è«–ç†æ§‹æˆã‚’ä¿ã¤

ã€è‹±èªãƒ†ã‚­ã‚¹ãƒˆã€‘
{request.text}

ã€æ—¥æœ¬èªç¿»è¨³è¦ç´„ã€‘
"""
        
        base_prompts = {
            TaskType.EXPAND: f"""
ä»¥ä¸‹ã®çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’è©³ç´°ã§å…·ä½“çš„ãªæ–‡ç« ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚
ç›®æ¨™æ–‡å­—æ•°: {request.max_length or 500}æ–‡å­—ç¨‹åº¦
å…ƒã®æ„å‘³ã‚’ä¿ã¡ãªãŒã‚‰ã€èƒŒæ™¯æƒ…å ±ã‚„è©³ç´°èª¬æ˜ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ:
{request.text}

å±•é–‹ã•ã‚ŒãŸæ–‡ç« :
""",
            TaskType.ANALYZE: f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã€è«–ç‚¹ã€å«æ„ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{request.text}

åˆ†æçµæœ:
"""
        }
        
        return base_prompts.get(request.task_type, request.text)
    
    def _mock_response(self, request: ApertusRequest) -> ApertusResponse:
        """ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆï¼ˆAPI ã‚­ãƒ¼ãŒç„¡ã„å ´åˆï¼‰"""
        time.sleep(1)  # å®Ÿéš›ã®APIé…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        if request.task_type == TaskType.SUMMARIZE:
            # è¨€èªåˆ¤å®šï¼ˆæ—¥æœ¬èªæ–‡å­—ãŒ50æ–‡å­—ä»¥ä¸Šã‚ã‚Œã°æ—¥æœ¬èªï¼‰
            japanese_chars = sum(1 for c in request.text[:1000] if ord(c) > 0x3000)
            is_japanese = japanese_chars > 50
            
            if is_japanese:
                # æ—¥æœ¬èªè¦ç´„
                sentences = request.text.split('ã€‚')
                summary_sentences = []
                current_length = 0
                max_len = request.max_length or 200
                
                for sentence in sentences:
                    if sentence.strip():
                        sentence = sentence.strip() + 'ã€‚'
                        if current_length + len(sentence) <= max_len:
                            summary_sentences.append(sentence)
                            current_length += len(sentence)
                        else:
                            break
                
                result = ''.join(summary_sentences)
                if not result:
                    result = request.text[:max_len] + "..." if len(request.text) > max_len else request.text
            else:
                # è‹±èªãƒ†ã‚­ã‚¹ãƒˆ â†’ æ—¥æœ¬èªç¿»è¨³è¦ç´„
                total_chars = len(request.text)
                mock_summary = f"""ã€Apertusãƒ¢ãƒƒã‚¯ç¿»è¨³è¦ç´„ï¼ˆè‹±èªâ†’æ—¥æœ¬èªï¼‰ã€‘
ğŸ“Š å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {total_chars:,}æ–‡å­—ï¼ˆè‹±èªï¼‰

æœ¬è«–æ–‡ã§ã¯ã€é‡è¦ãªç ”ç©¶ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«è«–ã˜ã¦ã„ã¾ã™ã€‚ç ”ç©¶è€…ã‚‰ã¯ç‰¹å®šã®æ‰‹æ³•ã‚’ç”¨ã„ã¦å®Ÿé¨“ã‚’è¡Œã„ã€èˆˆå‘³æ·±ã„çŸ¥è¦‹ã‚’å¾—ã¾ã—ãŸã€‚å¾—ã‚‰ã‚ŒãŸçµæœã¯ã€å½“è©²åˆ†é‡ã«ãŠã„ã¦é‡è¦ãªæ„ç¾©ã‚’æŒã¤ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ å®Ÿéš›ã®Apertus APIä½¿ç”¨æ™‚:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Apertusã¯è¤‡æ•°ã®é«˜æ€§èƒ½AIãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-4, Claude 3, Gemini Proï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€è‹±èªè«–æ–‡ã‚’æ­£ç¢ºã«ç†è§£ã—ã€è‡ªç„¶ãªæ—¥æœ¬èªã§ç¿»è¨³è¦ç´„ã—ã¾ã™ã€‚

ã€ä½¿ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‘
- GPT-4 Turbo: æœ€é«˜å“è³ªã®ç¿»è¨³ãƒ»è¦ç´„
- Claude 3 Sonnet: é•·æ–‡ç†è§£ã«å„ªã‚Œã‚‹
- Gemini Pro: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé«˜é€Ÿå‡¦ç†

â€» APERTUS_API_KEYã‚’è¨­å®šã™ã‚‹ã¨ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"""
                result = mock_summary
                
        elif request.task_type == TaskType.EXPAND:
            # ç°¡å˜ãªå±•é–‹ãƒ­ã‚¸ãƒƒã‚¯
            base_expansion = f"{request.text}ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã™ã‚‹ã¨ã€ã“ã‚Œã¯ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªè¦ç´ ã®ä¸€ã¤ã§ã™ã€‚"
            base_expansion += "ã“ã®æ¦‚å¿µã¯å¤šè§’çš„ãªè¦–ç‚¹ã‹ã‚‰ç†è§£ã™ã‚‹ã“ã¨ãŒã§ãã€æ§˜ã€…ãªåˆ†é‡ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚"
            base_expansion += "ã•ã‚‰ã«è©³ç´°ãªåˆ†æã‚’è¡Œã†ã¨ã€ãã®èƒŒæ™¯ã«ã¯è¤‡æ•°ã®è¦å› ãŒé–¢ä¿‚ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚"
            
            target_len = request.max_length or 500
            result = base_expansion[:target_len] if len(base_expansion) > target_len else base_expansion
            
        else:
            result = f"[Mock] {request.task_type.value} result for: {request.text[:100]}..."
        
        return ApertusResponse(
            success=True,
            result=result,
            model_used=f"mock-{request.model.value}",
            execution_time=1.0,
            token_usage={"input": len(request.text), "output": len(result)},
            confidence=0.85,
            metadata={"mock": True, "timestamp": time.time()}
        )
    
    def process_text(self, request: ApertusRequest) -> ApertusResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆApertus LLMä½¿ç”¨ï¼‰"""
        start_time = time.time()
        
        try:
            if self.mock_mode:
                return self._mock_response(request)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            prompt = self._create_prompt(request)
            
            # Apertus LLMã§å‡¦ç†
            if request.task_type == TaskType.SUMMARIZE:
                # è¦ç´„ã‚¿ã‚¹ã‚¯
                max_len = request.max_length or 400
                response = self.apertus.summarize(
                    text=request.text,
                    max_length=max_len
                )
            elif request.task_type == TaskType.EXPAND:
                # å±•é–‹ã‚¿ã‚¹ã‚¯
                target_len = request.max_length or 500
                response = self.apertus.expand(
                    text=request.text,
                    target_length=target_len,
                    prompt_template=prompt if request.custom_prompt else None
                )
            else:
                # ãã®ä»–ã®ã‚¿ã‚¹ã‚¯ï¼ˆæ±ç”¨ç”Ÿæˆï¼‰
                response = self.apertus.generate(
                    prompt=prompt,
                    max_new_tokens=request.max_length or 512,
                    temperature=request.temperature
                )
            
            execution_time = time.time() - start_time
            
            # ApertusResponseã«å¤‰æ›
            if response.success:
                return ApertusResponse(
                    success=True,
                    result=response.result,
                    model_used=response.model_used,
                    execution_time=execution_time,
                    token_usage=response.token_usage,
                    confidence=response.confidence,
                    metadata={
                        "timestamp": time.time(),
                        "task_type": request.task_type.value,
                        "apertus_version": "8B-Instruct-2509"
                    }
                )
            else:
                # Apertus LLMã‹ã‚‰ã®ã‚¨ãƒ©ãƒ¼
                return ApertusResponse(
                    success=False,
                    result="",
                    model_used=response.model_used or "unknown",
                    execution_time=execution_time,
                    token_usage={},
                    confidence=0.0,
                    metadata={"error_timestamp": time.time()},
                    error=response.error or "Unknown error from Apertus LLM"
                )
            
        except Exception as e:
            logger.error(f"Apertuså‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            execution_time = time.time() - start_time
            
            return ApertusResponse(
                success=False,
                result="",
                model_used=request.model.value,
                execution_time=execution_time,
                token_usage={},
                confidence=0.0,
                metadata={"error_timestamp": time.time()},
                error=str(e)
            )
    
    def summarize(self, text: str, max_length: int = 200, min_length: int = 50, 
                  model: ModelType = None) -> ApertusResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„"""
        request = ApertusRequest(
            text=text,
            task_type=TaskType.SUMMARIZE,
            model=model or self.default_model,
            max_length=max_length,
            min_length=min_length
        )
        return self.process_text(request)
    
    def expand(self, text: str, target_length: int = 500, 
               model: ModelType = None) -> ApertusResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆå±•é–‹"""
        request = ApertusRequest(
            text=text,
            task_type=TaskType.EXPAND,
            model=model or self.default_model,
            max_length=target_length
        )
        return self.process_text(request)
    
    def analyze(self, text: str, model: ModelType = None) -> ApertusResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
        request = ApertusRequest(
            text=text,
            task_type=TaskType.ANALYZE,
            model=model or self.default_model
        )
        return self.process_text(request)
    
    def health_check(self) -> bool:
        """Apertus LLMã®çŠ¶æ…‹ç¢ºèª"""
        if self.mock_mode:
            return True
        
        try:
            status = self.apertus.get_status()
            return status.success and self.apertus.available
        except:
            return False
    
    def get_available_models(self) -> List[str]:
        """ä½¿ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—"""
        if self.mock_mode:
            return [model.value for model in ModelType]
        
        # Apertus LLMä½¿ç”¨æ™‚
        if self.apertus.available:
            return ["swiss-ai/Apertus-8B-Instruct-2509"]
        else:
            return []