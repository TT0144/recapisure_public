#!/usr/bin/env python3
"""
Apertus Learning System
è¦ç´„ãƒ»ç¿»è¨³ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ã—ã¦ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import time
import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path

from .apertus_client import ApertusClient, ModelType, TaskType, ApertusRequest

logger = logging.getLogger(__name__)


@dataclass
class FeedbackScore:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢"""
    task_id: str
    original_text: str
    result_text: str
    user_score: float  # 1-5ã®è©•ä¾¡
    accuracy_score: float  # æ­£ç¢ºæ€§
    fluency_score: float  # æµæš¢æ€§
    completeness_score: float  # å®Œå…¨æ€§
    timestamp: str
    task_type: str  # summarize/translate/expand
    model_used: str
    user_feedback: Optional[str] = None


@dataclass
class LearningMetrics:
    """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    total_tasks: int
    average_score: float
    accuracy_trend: List[float]  # æ™‚ç³»åˆ—ã®æ­£ç¢ºæ€§
    fluency_trend: List[float]  # æ™‚ç³»åˆ—ã®æµæš¢æ€§
    best_score: float
    worst_score: float
    improvement_rate: float  # æ”¹å–„ç‡


class ApertusLearningSystem:
    """
    Apertusã‚’ä½¿ã£ãŸå­¦ç¿’å‹è¦ç´„ãƒ»ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åé›†
    - ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ãƒ»åˆ†æ
    - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‹•çš„ã«æœ€é©åŒ–
    - ç²¾åº¦ã‚’ç¶™ç¶šçš„ã«å‘ä¸Š
    """
    
    def __init__(self, apertus_client: Optional[ApertusClient] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            apertus_client: Apertusã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        """
        self.client = apertus_client or ApertusClient()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.data_dir = Path("data/learning")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        self.feedback_file = self.data_dir / "feedback_scores.jsonl"
        self.metrics_file = self.data_dir / "learning_metrics.json"
        
        # ãƒ¡ãƒ¢ãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.feedback_history: List[FeedbackScore] = []
        self.metrics: Optional[LearningMetrics] = None
        
        # å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.learned_params = self._load_learned_params()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        self._load_feedback_history()
        self._calculate_metrics()
        
        logger.info(f"âœ… å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†: {len(self.feedback_history)}ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯")
    
    def _load_learned_params(self) -> Dict[str, Any]:
        """å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"""
        param_file = self.data_dir / "learned_params.json"
        
        if param_file.exists():
            with open(param_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        return {
            "temperature": 0.7,
            "max_length_ratio": 0.3,  # è¦ç´„æ™‚ã®å…ƒãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã™ã‚‹é•·ã•æ¯”ç‡
            "min_length_ratio": 0.1,
            "prompt_style": "default",  # default/detailed/concise
            "translation_formality": "neutral",  # formal/neutral/casual
        }
    
    def _save_learned_params(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        param_file = self.data_dir / "learned_params.json"
        with open(param_file, 'w', encoding='utf-8') as f:
            json.dump(self.learned_params, f, ensure_ascii=False, indent=2)
    
    def _load_feedback_history(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if not self.feedback_file.exists():
            return
        
        with open(self.feedback_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    self.feedback_history.append(FeedbackScore(**data))
    
    def _save_feedback(self, feedback: FeedbackScore):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜"""
        with open(self.feedback_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(asdict(feedback), ensure_ascii=False) + '\n')
    
    def _calculate_metrics(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        if not self.feedback_history:
            self.metrics = LearningMetrics(
                total_tasks=0,
                average_score=0.0,
                accuracy_trend=[],
                fluency_trend=[],
                best_score=0.0,
                worst_score=0.0,
                improvement_rate=0.0
            )
            return
        
        scores = [fb.user_score for fb in self.feedback_history]
        accuracy_scores = [fb.accuracy_score for fb in self.feedback_history]
        fluency_scores = [fb.fluency_score for fb in self.feedback_history]
        
        # æ”¹å–„ç‡è¨ˆç®—ï¼ˆæœ€åˆã®10ä»¶ã¨æœ€æ–°10ä»¶ã‚’æ¯”è¼ƒï¼‰
        improvement_rate = 0.0
        if len(scores) >= 20:
            early_avg = sum(scores[:10]) / 10
            recent_avg = sum(scores[-10:]) / 10
            improvement_rate = ((recent_avg - early_avg) / early_avg) * 100
        
        self.metrics = LearningMetrics(
            total_tasks=len(self.feedback_history),
            average_score=sum(scores) / len(scores),
            accuracy_trend=accuracy_scores[-50:],  # ç›´è¿‘50ä»¶
            fluency_trend=fluency_scores[-50:],
            best_score=max(scores),
            worst_score=min(scores),
            improvement_rate=improvement_rate
        )
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜
        with open(self.metrics_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self.metrics), f, ensure_ascii=False, indent=2)
    
    def add_feedback(
        self,
        task_id: str,
        original_text: str,
        result_text: str,
        user_score: float,
        accuracy_score: float,
        fluency_score: float,
        completeness_score: float,
        task_type: str,
        model_used: str,
        user_feedback: Optional[str] = None
    ) -> bool:
        """
        ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¿½åŠ 
        
        Args:
            task_id: ã‚¿ã‚¹ã‚¯ID
            original_text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            result_text: ç”Ÿæˆçµæœ
            user_score: ãƒ¦ãƒ¼ã‚¶ãƒ¼è©•ä¾¡ (1-5)
            accuracy_score: æ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢ (0-100)
            fluency_score: æµæš¢æ€§ã‚¹ã‚³ã‚¢ (0-100)
            completeness_score: å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ (0-100)
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ— (summarize/translate/expand)
            model_used: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            user_feedback: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆ
        
        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        try:
            feedback = FeedbackScore(
                task_id=task_id,
                original_text=original_text[:500],  # ä¿å­˜å®¹é‡å‰Šæ¸›ã®ãŸã‚æœ€å¤§500æ–‡å­—
                result_text=result_text[:500],
                user_score=user_score,
                accuracy_score=accuracy_score,
                fluency_score=fluency_score,
                completeness_score=completeness_score,
                timestamp=datetime.now().isoformat(),
                task_type=task_type,
                model_used=model_used,
                user_feedback=user_feedback
            )
            
            # ãƒ¡ãƒ¢ãƒªã«è¿½åŠ 
            self.feedback_history.append(feedback)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            self._save_feedback(feedback)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å†è¨ˆç®—
            self._calculate_metrics()
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (10ä»¶ã”ã¨)
            if len(self.feedback_history) % 10 == 0:
                self._optimize_params_from_feedback()
            
            logger.info(f"âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½åŠ : task_id={task_id}, score={user_score}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_metrics(self) -> LearningMetrics:
        """ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        if self.metrics is None:
            self._calculate_metrics()
        return self.metrics
    
    def _optimize_params_from_feedback(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–"""
        if len(self.feedback_history) < 10:
            return  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯æœ€é©åŒ–ã—ãªã„
        
        recent_feedback = self.feedback_history[-50:]  # ç›´è¿‘50ä»¶
        
        # é«˜è©•ä¾¡ã®ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´ã‚’åˆ†æ
        high_score_tasks = [fb for fb in recent_feedback if fb.user_score >= 4.0]
        
        if high_score_tasks:
            # é«˜è©•ä¾¡ã‚¿ã‚¹ã‚¯ã®å¹³å‡é•·ã•æ¯”ç‡ã‚’è¨ˆç®—
            # ï¼ˆã“ã“ã§ã¯ç°¡æ˜“çš„ãªå®Ÿè£…ã€‚å®Ÿéš›ã¯ã‚ˆã‚Šè©³ç´°ãªåˆ†æãŒå¯èƒ½ï¼‰
            avg_accuracy = sum(fb.accuracy_score for fb in high_score_tasks) / len(high_score_tasks)
            avg_fluency = sum(fb.fluency_score for fb in high_score_tasks) / len(high_score_tasks)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
            if avg_accuracy > 4.0 and avg_fluency > 4.0:
                # é«˜å“è³ªãªã®ã§ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¶­æŒ
                pass
            elif avg_accuracy < 3.5:
                # æ­£ç¢ºæ€§ãŒä½ã„ â†’ ã‚ˆã‚Šè©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«
                self.learned_params["prompt_style"] = "detailed"
                self.learned_params["max_length_ratio"] = min(0.4, self.learned_params["max_length_ratio"] + 0.05)
            elif avg_fluency < 3.5:
                # æµæš¢æ€§ãŒä½ã„ â†’ ã‚ˆã‚Šç°¡æ½”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«
                self.learned_params["prompt_style"] = "concise"
                self.learned_params["temperature"] = min(0.9, self.learned_params["temperature"] + 0.1)
            
            self._save_learned_params()
            logger.info(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Œäº†: {self.learned_params}")
    
    def _get_optimized_prompt(self, text: str, task_type: TaskType) -> str:
        """å­¦ç¿’çµæœã‚’åæ˜ ã—ãŸæœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        style = self.learned_params["prompt_style"]
        
        if task_type == TaskType.SUMMARIZE:
            max_len = int(len(text) * self.learned_params["max_length_ratio"])
            min_len = int(len(text) * self.learned_params["min_length_ratio"])
            
            if style == "detailed":
                return f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{min_len}æ–‡å­—ä»¥ä¸Š{max_len}æ–‡å­—ä»¥ä¸‹ã§è©³ç´°ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦æ±‚äº‹é …ã€‘
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å…¨ã¦å«ã‚ã‚‹
- å…·ä½“çš„ãªæ•°å€¤ã‚„å›ºæœ‰åè©ã‚’ä¿æŒ
- è«–ç†çš„ãªæµã‚Œã‚’ç¶­æŒ
- èª­ã¿ã‚„ã™ãè‡ªç„¶ãªæ—¥æœ¬èªã§

ã€ãƒ†ã‚­ã‚¹ãƒˆã€‘
{text}

ã€è¦ç´„ã€‘
"""
            elif style == "concise":
                return f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_len}æ–‡å­—ä»¥ä¸‹ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®ã¿ã‚’æŠ½å‡ºã—ã€æ˜ç¢ºã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

è¦ç´„:
"""
            else:  # default
                return f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{min_len}æ–‡å­—ä»¥ä¸Š{max_len}æ–‡å­—ä»¥ä¸‹ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ¼ã‚‰ã•ãšã€ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

è¦ç´„:
"""
        
        elif task_type == TaskType.TRANSLATE:
            formality = self.learned_params["translation_formality"]
            
            formality_instruction = {
                "formal": "ä¸å¯§ã§æ ¼å¼é«˜ã„æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚",
                "neutral": "è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚",
                "casual": "è¦ªã—ã¿ã‚„ã™ãåˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"
            }
            
            return f"""
ä»¥ä¸‹ã®è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³æ–¹é‡ã€‘
{formality_instruction[formality]}
å°‚é–€ç”¨èªã¯é©åˆ‡ãªæ—¥æœ¬èªè¨³ã‚’ä½¿ç”¨ã—ã€å¿…è¦ã«å¿œã˜ã¦è‹±èªã‚’ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚
åŸæ–‡ã®æ„å›³ã¨ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’æ­£ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚

ã€è‹±èªãƒ†ã‚­ã‚¹ãƒˆã€‘
{text}

ã€æ—¥æœ¬èªç¿»è¨³ã€‘
"""
        
        return text
    
    def process_with_learning(
        self, 
        text: str, 
        task_type: TaskType, 
        model: ModelType = ModelType.GPT4_TURBO
    ) -> Tuple[Dict[str, Any], str]:
        """
        å­¦ç¿’æ©Ÿèƒ½ä»˜ãã§ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
        
        Args:
            text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            
        Returns:
            (çµæœè¾æ›¸, ã‚¿ã‚¹ã‚¯ID)
        """
        # ã‚¿ã‚¹ã‚¯IDç”Ÿæˆ
        task_id = f"{task_type.value}_{int(time.time() * 1000)}"
        
        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = self._get_optimized_prompt(text, task_type)
        
        # Apertus APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        request = ApertusRequest(
            text=text,
            task_type=task_type,
            model=model,
            temperature=self.learned_params["temperature"],
            custom_prompt=prompt
        )
        
        start_time = time.time()
        response = self.client.process_text(request)
        execution_time = time.time() - start_time
        
        result = {
            'success': response.success,
            'result': response.result,
            'task_id': task_id,
            'model_used': response.model_used,
            'execution_time': execution_time,
            'confidence': response.confidence,
            'learned_params_used': self.learned_params.copy(),
            'total_learning_samples': len(self.feedback_history),
            'average_score': self.metrics.average_score if self.metrics else 0.0,
        }
        
        if not response.success:
            result['error'] = response.error
        
        return result, task_id
    
    def submit_feedback(
        self,
        task_id: str,
        original_text: str,
        result_text: str,
        user_score: float,
        accuracy_score: float,
        fluency_score: float,
        completeness_score: float,
        task_type: str,
        model_used: str,
        user_feedback: Optional[str] = None
    ):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡
        
        Args:
            task_id: ã‚¿ã‚¹ã‚¯ID
            original_text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
            result_text: çµæœãƒ†ã‚­ã‚¹ãƒˆ
            user_score: ç·åˆè©•ä¾¡ (1-5)
            accuracy_score: æ­£ç¢ºæ€§ (1-5)
            fluency_score: æµæš¢æ€§ (1-5)
            completeness_score: å®Œå…¨æ€§ (1-5)
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            model_used: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            user_feedback: ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        """
        feedback = FeedbackScore(
            task_id=task_id,
            original_text=original_text[:500],  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            result_text=result_text[:1000],
            user_score=user_score,
            accuracy_score=accuracy_score,
            fluency_score=fluency_score,
            completeness_score=completeness_score,
            timestamp=datetime.now().isoformat(),
            task_type=task_type,
            model_used=model_used,
            user_feedback=user_feedback
        )
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¿å­˜
        self.feedback_history.append(feedback)
        self._save_feedback(feedback)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å†è¨ˆç®—
        self._calculate_metrics()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼ˆ10ä»¶ã”ã¨ï¼‰
        if len(self.feedback_history) % 10 == 0:
            self._optimize_params_from_feedback()
            logger.info(f"ğŸ“ å­¦ç¿’å®Œäº†: {len(self.feedback_history)}ä»¶ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰æœ€é©åŒ–")
        
        logger.info(f"âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å—ä¿¡: ã‚¿ã‚¹ã‚¯{task_id}, ã‚¹ã‚³ã‚¢{user_score}/5")
        
        return feedback  # è¿½åŠ : ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
    
    def get_metrics(self) -> LearningMetrics:
        """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        if not self.metrics:
            self._calculate_metrics()
        return self.metrics
    
    def get_learning_status(self) -> Dict[str, Any]:
        """å­¦ç¿’çŠ¶æ…‹ã‚’å–å¾—"""
        if not self.metrics:
            return {"status": "no_data"}
        
        return {
            "total_feedback": self.metrics.total_tasks,
            "average_score": round(self.metrics.average_score, 2),
            "best_score": round(self.metrics.best_score, 2),
            "worst_score": round(self.metrics.worst_score, 2),
            "improvement_rate": round(self.metrics.improvement_rate, 2),
            "current_params": self.learned_params,
            "recent_accuracy_avg": round(sum(self.metrics.accuracy_trend[-10:]) / 10, 2) if self.metrics.accuracy_trend else 0,
            "recent_fluency_avg": round(sum(self.metrics.fluency_trend[-10:]) / 10, 2) if self.metrics.fluency_trend else 0,
        }
    
    def get_performance_chart_data(self) -> Dict[str, List]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if not self.feedback_history:
            return {"labels": [], "scores": [], "accuracy": [], "fluency": []}
        
        recent = self.feedback_history[-50:]  # ç›´è¿‘50ä»¶
        
        return {
            "labels": [f"#{i+1}" for i in range(len(recent))],
            "scores": [fb.user_score for fb in recent],
            "accuracy": [fb.accuracy_score for fb in recent],
            "fluency": [fb.fluency_score for fb in recent],
        }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_learning_system = None

def get_learning_system(apertus_client: Optional[ApertusClient] = None) -> ApertusLearningSystem:
    """å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—"""
    global _learning_system
    if _learning_system is None:
        _learning_system = ApertusLearningSystem(apertus_client)
    return _learning_system
