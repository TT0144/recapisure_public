#!/usr/bin/env python3
"""
Apertus LLM Service
ã‚¹ã‚¤ã‚¹æ”¿åºœè£½ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLM (swiss-ai/Apertus-8B-Instruct-2509)
ã¾ãŸã¯è»½é‡ä»£æ›¿ãƒ¢ãƒ‡ãƒ« (rinna/japanese-gpt2-medium)
"""

import os
import logging
import time
from typing import Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# Apertusåˆ©ç”¨å¯èƒ½ãƒã‚§ãƒƒã‚¯
APERTUS_AVAILABLE = False
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    APERTUS_AVAILABLE = True
    logger.info("âœ… Apertus LLMåˆ©ç”¨å¯èƒ½")
except ImportError as e:
    logger.warning(f"âš ï¸ Apertus LLMåˆ©ç”¨ä¸å¯: {e}")


# ğŸ‡¨ğŸ‡­ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«å®šç¾©
AVAILABLE_MODELS = {
    "apertus-8b": {
        "model_id": "swiss-ai/Apertus-8B-Instruct-2509",
        "name": "Apertus 8B (Swiss AI)",
        "size": "8B parameters",
        "requires_gpu": True,
        "memory_gb": 16,
        "description": "ã‚¹ã‚¤ã‚¹æ”¿åºœè£½ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLM (é«˜æ€§èƒ½)"
    },
    "gpt2-small": {
        "model_id": "gpt2",  # OpenAI GPT-2 (å‹•ä½œç¢ºèªæ¸ˆã¿)
        "name": "GPT-2 Small",
        "size": "124M parameters",
        "requires_gpu": False,
        "memory_gb": 1,
        "description": "è»½é‡æ±ç”¨ãƒ¢ãƒ‡ãƒ« (CPUå‹•ä½œå¯ã€å‹•ä½œç¢ºèªæ¸ˆã¿)"
    },
    "rinna-bilingual": {
        "model_id": "rinna/bilingual-gpt-neox-4b",
        "name": "Rinna Bilingual 4B",
        "size": "4B parameters",
        "requires_gpu": True,
        "memory_gb": 8,
        "description": "æ—¥è‹±ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ãƒ¢ãƒ‡ãƒ« (ä¸­æ€§èƒ½)"
    }
}


def get_recommended_model() -> str:
    """
    ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã«å¿œã˜ãŸæ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™
    
    Returns:
        æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ID
    """
    try:
        import torch
        if torch.cuda.is_available():
            # GPUåˆ©ç”¨å¯èƒ½ â†’ Apertus 8Bã‚’æ¨å¥¨
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_memory >= 16:
                logger.info("ğŸ¯ æ¨å¥¨: Apertus 8B (GPU 16GB+)")
                return "apertus-8b"
            elif gpu_memory >= 8:
                logger.info("ğŸ¯ æ¨å¥¨: Rinna Bilingual 4B (GPU 8GB+)")
                return "rinna-bilingual"
        
        # CPUç’°å¢ƒ â†’ è»½é‡ãƒ¢ãƒ‡ãƒ«
        logger.info("ğŸ¯ æ¨å¥¨: GPT-2 Small (CPU)")
        return "gpt2-small"
    except:
        return "gpt2-small"


@dataclass
class ApertusResponse:
    """Apertusãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool
    result: str
    model_used: str
    execution_time: float = 0.0
    confidence: float = 0.90
    token_usage: Dict[str, int] = None
    error: Optional[str] = None


class ApertusService:
    """Apertus AI Service (ã‚¹ã‚¤ã‚¹æ”¿åºœè£½8B LLM)"""
    
    def __init__(self, model_name: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«æŒ‡å®š (None=è‡ªå‹•é¸æŠ, "apertus-8b", "rinna-medium", etc.)
        """
        # ãƒ¢ãƒ‡ãƒ«è‡ªå‹•é¸æŠ
        if model_name is None:
            model_key = get_recommended_model()
            model_name = AVAILABLE_MODELS[model_key]["model_id"]
            logger.info(f"ğŸ¤– è‡ªå‹•é¸æŠ: {AVAILABLE_MODELS[model_key]['name']}")
        elif model_name in AVAILABLE_MODELS:
            # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆåã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—
            model_name = AVAILABLE_MODELS[model_name]["model_id"]
        
        self.model_name = model_name
        self.available = APERTUS_AVAILABLE
        self.model = None
        self.tokenizer = None
        self.loaded = False
        
        # 4bité‡å­åŒ–è¨­å®š
        self.use_4bit = True  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚4bité‡å­åŒ–ã‚’ä½¿ç”¨
        
        # Apertusäº’æ›ãƒ¢ãƒ¼ãƒ‰
        self.is_apertus_compatible = "apertus" in model_name.lower() or "swiss" in model_name.lower()
        
        logger.info(f"ğŸ‡¨ğŸ‡­ Apertus ServiceåˆæœŸåŒ–: {model_name}")
        logger.info(f"   4bité‡å­åŒ–: {'æœ‰åŠ¹' if self.use_4bit else 'ç„¡åŠ¹'}")
        logger.info(f"   Apertusäº’æ›: {'ã¯ã„' if self.is_apertus_compatible else 'ã„ã„ãˆ (ä»£æ›¿ãƒ¢ãƒ‡ãƒ«)'}")
    
    def load_model(self) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (åˆå›ã®ã¿)
        
        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        if self.loaded:
            return True
        
        if not self.available:
            logger.error("âŒ Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        try:
            logger.info(f"ğŸ“¥ Apertusãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... ({self.model_name})")
            start_time = time.time()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            if self.use_4bit:
                # 4bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰ (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)
                logger.info("   ğŸ”§ 4bité‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰ã§ãƒ­ãƒ¼ãƒ‰ä¸­...")
                
                try:
                    from transformers import BitsAndBytesConfig
                    
                    # 4bité‡å­åŒ–è¨­å®š
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4"
                    )
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        quantization_config=quantization_config,
                        device_map="auto"
                    )
                    logger.info("   âœ… 4bité‡å­åŒ–ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                    
                except ImportError:
                    logger.warning("   âš ï¸ bitsandbytesãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚é€šå¸¸ãƒ­ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    self.use_4bit = False
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float16,
                        device_map="auto"
                    )
            else:
                # é€šå¸¸ãƒ­ãƒ¼ãƒ‰ (FP16)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
            
            load_time = time.time() - start_time
            self.loaded = True
            
            logger.info(f"âœ… Apertusãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† ({load_time:.1f}ç§’)")
            logger.info(f"   ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
            logger.info(f"   é‡å­åŒ–: {'4bit' if self.use_4bit else 'FP16'}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Apertusãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> ApertusResponse:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_new_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0.0-1.0)
            top_p: Top-p sampling (0.0-1.0)
            do_sample: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            ApertusResponse
        """
        if not self.available:
            return ApertusResponse(
                success=False,
                result="",
                model_used="unavailable",
                error="Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            )
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (åˆå›ã®ã¿)
        if not self.loaded:
            if not self.load_model():
                return ApertusResponse(
                    success=False,
                    result="",
                    model_used=self.model_name,
                    error="ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
                )
        
        try:
            start_time = time.time()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            if hasattr(self.model, 'device'):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if generated_text.startswith(prompt):
                result = generated_text[len(prompt):].strip()
            else:
                result = generated_text
            
            execution_time = time.time() - start_time
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
            input_tokens = inputs['input_ids'].shape[1]
            output_tokens = outputs.shape[1]
            
            logger.info(f"âœ… Apertusç”Ÿæˆå®Œäº† ({execution_time:.1f}ç§’)")
            logger.info(f"   å…¥åŠ›: {input_tokens} tokens, å‡ºåŠ›: {output_tokens} tokens")
            
            return ApertusResponse(
                success=True,
                result=result,
                model_used=self.model_name,
                execution_time=execution_time,
                confidence=0.92,
                token_usage={
                    "prompt_tokens": input_tokens,
                    "completion_tokens": output_tokens - input_tokens,
                    "total_tokens": output_tokens
                }
            )
            
        except Exception as e:
            logger.error(f"âŒ Apertusç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ApertusResponse(
                success=False,
                result="",
                model_used=self.model_name,
                error=str(e)
            )
    
    def summarize(self, text: str, max_length: int = 200) -> ApertusResponse:
        """
        ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„
        
        Args:
            text: è¦ç´„ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            max_length: æœ€å¤§æ–‡å­—æ•°
        
        Returns:
            ApertusResponse
        """
        prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_length}æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

è¦ç´„:"""
        
        return self.generate(
            prompt=prompt,
            max_new_tokens=max_length * 2,  # æ—¥æœ¬èªã¯1æ–‡å­—â‰ˆ2ãƒˆãƒ¼ã‚¯ãƒ³
            temperature=0.3,  # è¦ç´„ã¯æ±ºå®šçš„ã«
            do_sample=True
        )
    
    def expand(self, text: str, target_length: int = 500) -> ApertusResponse:
        """
        çŸ­æ–‡å±•é–‹
        
        Args:
            text: å±•é–‹ã™ã‚‹çŸ­æ–‡
            target_length: ç›®æ¨™æ–‡å­—æ•°
        
        Returns:
            ApertusResponse
        """
        prompt = f"""ä»¥ä¸‹ã®çŸ­ã„æ–‡ç« ã‚’ã€ç´„{target_length}æ–‡å­—ç¨‹åº¦ã®è©³ç´°ãªæ–‡ç« ã«å±•é–‹ã—ã¦ãã ã•ã„ã€‚

å…ƒã®æ–‡ç« :
{text}

è©³ç´°ãªæ–‡ç« :"""
        
        return self.generate(
            prompt=prompt,
            max_new_tokens=target_length * 2,
            temperature=0.7,
            do_sample=True
        )
    
    def get_status(self) -> ApertusResponse:
        """ã‚µãƒ¼ãƒ“ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
        model_info = None
        for key, info in AVAILABLE_MODELS.items():
            if info["model_id"] == self.model_name:
                model_info = info
                break
        
        status_info = {
            'service': 'Apertus LLM',
            'model_name': self.model_name,
            'model_display_name': model_info['name'] if model_info else self.model_name,
            'model_size': model_info['size'] if model_info else 'Unknown',
            'available': self.available,
            'loaded': self.loaded,
            'quantization': '4bit' if self.use_4bit else 'FP16',
            'device': 'auto',
            'api_key_required': False,
            'completely_free': True,
            'developer': 'Swiss AI (ã‚¹ã‚¤ã‚¹æ”¿åºœ)' if self.is_apertus_compatible else 'Rinna Co., Ltd.',
            'is_apertus_official': self.is_apertus_compatible,
            'memory_requirement_gb': model_info['memory_gb'] if model_info else 'Unknown'
        }
        
        return ApertusResponse(
            success=True,
            result=status_info,
            model_used=self.model_name,
            execution_time=0.0,
            confidence=1.0
        )
    
    def list_available_models(self) -> Dict[str, Any]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        return AVAILABLE_MODELS


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_apertus_service = None

def get_apertus_service() -> ApertusService:
    """ApertusServiceã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—"""
    global _apertus_service
    if _apertus_service is None:
        _apertus_service = ApertusService()
    return _apertus_service
